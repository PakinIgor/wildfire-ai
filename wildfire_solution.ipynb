{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели для wildfire.ai (https://wildfire.sberbank.ai/competition)\n",
    "\n",
    "Оставляю почти без изменений весь код, чтобы случайно что-то не поменять, поэтому тут далеко не все красиво - есть копипаста, не все переменные адекватно названы, лишние print'ы и т.п....\n",
    "\n",
    "Весь конкурс я работал на ноуте (Interl Core I5-8265U, 4 cores, 8 гб RAM), поэтому запуститься должно у всех\n",
    "\n",
    "Докер-образ использовал sberbank/mchs-python, который подготовили организаторы, версии библиотек можно подтянуть оттуда для воспроизводимости\n",
    "\n",
    "Поход по лидерборду был как-то так:\n",
    "* 617 фич (брал только из ncep + пожары за предыдущее время, без кластеризации и городов) - ***0.9187 паблик***\n",
    "* 611 фичей (убрал какие-то старые, добавил что-то, точно не помню) - ***0.9235 паблик***\n",
    "* 677 фичей (добавил кластеризацию, день в году и год убрал из категориальных) - ***0.9253 паблик***;\n",
    "* 228 фичей (нагенерил дополнительных фич (все которые в этом ноутбуке, кроме городов и лесов, полей, заповедников), отобрал по permutaion importance) - ***0.9289 паблик***\n",
    "* 236 фич, добавил города, месяц убрал из категориальнфх фич, немного поменял параметры lightgbm, добавил леса) - ***0.9297 паблик***;\n",
    "* 242 фич, добавил еще поля и заповедники - ***0.9299 паблик***\n",
    "* 215 фич (дополнительно убрал некоторые по permutation importance) - ***0.9303 паблик***\n",
    "* 214 фич (убрал месяц из фич) - ***0.9305 паблик***\n",
    "\n",
    "на привате выиграла последняя модель\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "import xarray\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('best_solution')\n",
    "from train_features import model_features # финальные фичи, отобранные при помощи permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('wildfires_train.csv')\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "df_train['timestamp'] = df_train['date'].apply(lambda x: time.mktime(x.timetuple()))\n",
    "df_train.set_index('fire_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_of_year(date):\n",
    "    '''\n",
    "        хотел на самом деле считать по-другому и в конце считал по другому, но в итоге попала эта версия\n",
    "        день в году:\n",
    "          1) если год не високосный, то просто день в году\n",
    "          2) если високосный - то если это 29 февраля, то полагаем день = 366, \n",
    "          если дата >= 1 марта, отнимаем 1 день (чтобы дни совпадали для високосного и невисокосных)\n",
    "    '''\n",
    "    doy = date.timetuple().tm_yday\n",
    "    if date.year in (2012,2016,2020):\n",
    "        if date >= datetime.date(date.year,3,1):\n",
    "            doy -= 1\n",
    "        elif date == datetime.date(date.year,2,29):\n",
    "            doy = 366\n",
    "    return doy\n",
    "\n",
    "def calc_bearing(lon1,lat1,lon2,lat2):\n",
    "    '''\n",
    "        угол между двумя точками\n",
    "    '''\n",
    "    delta_lon = lon2 - lon1\n",
    "    X = np.cos(math.radians(lat2)) * np.sin(math.radians(delta_lon))\n",
    "    Y = np.cos(math.radians(lat1)) * np.sin(math.radians(lat2)) - np.sin(math.radians(lat1)) * np.cos(math.radians(lat2)) * np.cos(math.radians(delta_lon))\n",
    "    bearing = math.atan2(X,Y)\n",
    "    return bearing * 180/math.pi\n",
    "\n",
    "def calc_distance(lon1,lat1,lon2,lat2):\n",
    "    '''  расстояние между двумя точками '''\n",
    "    radius = 6371302  # m\n",
    "\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = (math.sin(dlat / 2) * math.sin(dlat / 2) +\n",
    "         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *\n",
    "         math.sin(dlon / 2) * math.sin(dlon / 2))\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = radius * c\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ниже основной класс для генерации фич.\n",
    "\n",
    "По умолчанию взял который были на сервере - uwind,air,rhum - они в итоговой модели только и остались исторические.\n",
    "\n",
    "Еще взял vwind, но их брал только за определенную дату.\n",
    "\n",
    "Из других открытых источников:\n",
    "* города (брал отсюда https://gist.github.com/nalgeon/5307af065ff0e3bc97927c832fabe26b);\n",
    "* поля, леса, заповедники взял из решения команды Gardeners https://github.com/VovaForbes/WildFire \n",
    "\n",
    "# Основные идеи\n",
    "<ol>\n",
    "    <li>Так как размер шага в данных ncep очень большой (2.5x2.5 градуса), то вряд ли разные статистики (типа скользящих средний и т.д.) много накинут в качестве, так как реальные значения в конкретной точке могут быть совсем другие. Но если рассматривать квадраты с центром в узлах сетки ncep, то мы получаем много квадратных полигонов, на которые разбита Россия. В этом случае каждый полигон, помимо координат, можно охарактеризовать еще набором других каких-либо параметров - например, количество пожаров в полигоне, ncep данные за конкретную дату (я в итоге брал за 09.2019, 10.2019, 11.2019) и т.п. Главная идея была в том, что координаты - это две переменные, чтобы найти похожие несколько полигонов по пожарам, модели нужно сделать много итераций, и в этом случае мы помогаем модели этими дополнительными фичами.\n",
    "    Если я убирал эти фичи из рассмотрения, скор на валидации очень прилично падал (на паблике даже не проверял, больше доверял CV).</li>\n",
    "    <li> Если проверять статистику по пожарам, то из года в год она все-таки прилично меняется, хоть и корреляция какая-то есть (например, если брать в качестве предикта KNN за последние три года в диапазоне +- 21 день от текущего дня, auc получатся около 0.73). Поэтому я специально не стал брать разные таргет энкодинги. Поэтому из исторических я только взял такие фичи:\n",
    "        <ol>\n",
    "        <li>координаты и расстояние до ближайшего пожара +- 14 дней за последние 3 года (не считая текущего года) и общее количество найденных пожаров за этот период</li>\n",
    "            <li> общее количество пожаров за 3 года +- 7 дней от текущей даты</li>\n",
    "            <li>координаты до ближайшего пожара в тот же день на расстоянии 10, 25 км, расстояние до этого пожара (если такой есть) и угол между двумя точками</li>\n",
    "        </ol></li> \n",
    "    <li>Зато достаточно важная информация по размеру кластеров, в которых были пожары. Данные я только учитывал только за предыдущий год (очень важно это было учесть при кросс-валидации, эту фичу каждый раз приходилось делать заново), в итоге вышли такие фичи (алгоритм кластеризации DBSCAN):\n",
    "    <ol>\n",
    "        <li>для eps=0.035 (параметр DBSCAN) - количество пожаров в кластере, центр кластера (просто среднее по всем координатам), площадь внешнего прямоугольника, охватывающего весь кластер (вычисляется просто - ищутся в кластере макс, мин координаты, по ним площадь); </li>\n",
    "         <li>для eps=0.15 (параметр DBSCAN) - количество пожаров в кластере, центр кластера (просто среднее по всем координатам), площадь внешнего прямоугольника, охватывающего весь кластер (вычисляется просто - ищутся в кластере макс, мин координаты, по ним площадь); </li>    \n",
    "        <li> отношение площадей прямоугольников и количества в кластере для eps=0.035 и eps=0.15</li>\n",
    "        </ol>\n",
    "    <li> на основе ncep еще дополнительно формировал модуль скорости ветра wind_module ((uwind**2+vwind**2)**0.5) и направление ветра wind_direction (как считается можно посмотреть в wind_virection). Фичи: \n",
    "    <ol>\n",
    "        <li> значения в точке по uwind,air,rhum </li>\n",
    "        <li> среднее значение и среднее гармоническое по 4 ближайшим точкам из ncep для uwind,air,rhum </li>\n",
    "        <li> среднее взвешенное по расстоянию от точки до точек сетки по 4 ближайшим точкам из ncep для uwind,air,rhum </li>\n",
    "        <li>разные скользящие средние (7,14,21,35 дней) по всем доступным уровням (base_levels) по uwind,air,rhum; </li>\n",
    "        <li> с </li>\n",
    "        <li> разница между текущим значением и скользящим средним за 14 дней по uwind,air,rhum </li>\n",
    "        <li> значения в точке для uwind,vwind,wind_module,wind_direction,air,rhum за 01.11.2019 и за 01.09.2019</li>\n",
    "        <li> значения в точке скользящего среднего за 21 день для uwind,vwind,wind_module,wind_direction,air,rhum за 01.11.2019 </li>\n",
    "        <li> значения в точке скользящего среднего за 7 дней для uwind,vwind,wind_module,wind_direction,air,rhum за 01.10.2019 </li>\n",
    "        <li> среднее значение и среднее гармоническое по 4 ближайшим точкам из ncep для uwind,vwind,wind_module,wind_direction,air,rhum за 01.09.2019 и 01.11.2019 </li>\n",
    "         <li> разные разницы между значениями, полученными выше </li>       \n",
    "    </ol>\n",
    "    </li>\n",
    "    <li> из дат я нагенерил несколько фич, но в итоговой модели осталась только две - год и день в году (он считается немного хитро, чтобы учитывать високосный год) </li>\n",
    "    <li> расстояние до ближайшего города, население этого города и его координаты</li>\n",
    "    <li> расстояние до ближайшего поля, леса, заповедника и координаты</li>\n",
    "    \n",
    "для вычисления расстояний везде использовал KDTree - очень удобно и достаточно быстро;\n",
    "\n",
    "для кластеризации - DBSCAN (находит кластеры разной формы, для кластеризации по координатам фитится только eps, удобно)\n",
    "\n",
    "итоговая модель - lightgbm (бустинг), параметры подкрутил вручную\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesMaker():\n",
    "    def __init__(self):\n",
    "        self.lat_min = 32.5\n",
    "        self.lat_max = 75\n",
    "        self.lon_min = 12.5\n",
    "        self.lon_max = 182.5      \n",
    "        self.base_levels = (1000,700,500,300,250,200,100,150,70,50)\n",
    "        self.rhum_levels = (1000,850,700,500,400,300)\n",
    "        #self.base_levels = (1000,700,50,100)\n",
    "        self.ncep_data = None\n",
    "        self.lat_lons = np.array([x for x in itertools.product(np.arange(self.lon_min,self.lon_max+1,2.5),\\\n",
    "                                    np.arange(self.lat_min,self.lat_max+1,2.5))])\n",
    "        self.kd_tree = KDTree(self.lat_lons)\n",
    "    \n",
    "    def make_raw_features(self,df):\n",
    "        start = time.time()\n",
    "        print('Making raw features...')\n",
    "        basic_features = ['latitude','longitude']\n",
    "        features_df = df[['fire_id','latitude','longitude','date']].copy()\n",
    "        features_df['number_of_week'] = features_df['date'].apply(lambda x: x.isocalendar()[1])\n",
    "        features_df['day_of_year'] = features_df['date'].apply(get_day_of_year)\n",
    "        features_df['month'] = features_df['date'].apply(lambda x: x.month)\n",
    "        features_df['year'] = features_df['date'].apply(lambda x: x.year)\n",
    "        print('Making raw features done! Time {:.2f}'.format(time.time()-start))\n",
    "        return features_df.drop('date',axis=1).set_index('fire_id')\n",
    "    \n",
    "    def make_temp_rhum_features(self,df):\n",
    "        start = time.time()\n",
    "        print('Reading temperature-rhum-dataset...')\n",
    "        self.read_temp_rhum_set()\n",
    "        print(\"Reading dataset done! Time {:.2f}\".format(time.time()-start))\n",
    "        start = time.time()\n",
    "        print('Making temperature-humidity features...')\n",
    "        temp_rhum_features = self.extract_temp_rhum_features(df)\n",
    "        print(pd.DataFrame(temp_rhum_features).shape)\n",
    "        print('Making temperature-humidity features done! Time {:.2f}'.format(time.time()-start))\n",
    "        return pd.DataFrame(temp_rhum_features).set_index('fire_id')\n",
    "        \n",
    "    def wind_direction(self,u,v,wind_module):\n",
    "        try:\n",
    "            wind_dir_trig_to = np.arctan2(u/wind_module,  v/wind_module)* 180/math.pi\n",
    "        except:\n",
    "            wind_dir_trig_to = 0    \n",
    "        return wind_dir_trig_to\n",
    "            \n",
    "    def read_temp_rhum_set(self):        \n",
    "        ncep_data = []\n",
    "        for var in ('rhum','air'): #'uwnd','vwnd', 'rhum','trpp','srfpt'):\n",
    "            print(var)\n",
    "            for year in range(2012,2020,1):\n",
    "                dataset_filename = '{}.{}.nc'.format(var, year)\n",
    "                array = xarray.open_dataset(dataset_filename)\n",
    "                array = array.where((array.lat >= self.lat_min) &\n",
    "                      (array.lat <= self.lat_max) & \n",
    "                      (array.lon <= self.lon_max) & \n",
    "                      (array.lon >= self.lon_min) & \n",
    "                     (array.level.isin(self.base_levels)),drop=True)\n",
    "                ncep_data.append(array)\n",
    "            full_array = xarray.merge(ncep_data)\n",
    "        self.ncep_data = full_array\n",
    "        \n",
    "    def extract_temp_rhum_features(self,df):   \n",
    "        features_df = df[['fire_id','longitude','latitude','lat','lon','lat_near_2','lon_near_2','lat_near_3','lon_near_3','lat_near_4','lon_near_4',\n",
    "                                  'time']].copy()\n",
    "        features_df['bearing_from_nearest'] = features_df[['lon','lat','longitude','latitude']]\\\n",
    "                                   .apply(lambda x: calc_bearing(x['lon'],x['lat'],x['longitude'],x['latitude']),axis=1)\n",
    "        features_df['distance_from_nearest'] = features_df[['lon','lat','longitude','latitude']]\\\n",
    "                                   .apply(lambda x: calc_distance(x['lon'],x['lat'],x['longitude'],x['latitude']),axis=1)\n",
    "        \n",
    "        for level in self.base_levels:\n",
    "            point_df = self.ncep_data.sel(\n",
    "                level=level,\n",
    "            ).to_dataframe().reset_index().rename(columns={'air':'temperature_level{}'.format(level),\n",
    "                                                          'rhum':'humidity_level{}'.format(level)})\n",
    "            air = 'temperature_level{}'.format(level)\n",
    "            rhum = 'humidity_level{}'.format(level)\n",
    "            \n",
    "            if level == self.base_levels[0]:\n",
    "                print(point_df['time'].max(),point_df['time'].min())\n",
    "\n",
    "            p21d = point_df.groupby(['lat','lon']).rolling(window=21,min_periods=1,on='time')[[air,rhum]].mean().reset_index().\\\n",
    "                  rename(columns={air:'p21d-t_level{}'.format(level),rhum:'p21d-h_level{}'.format(level)})\n",
    "            \n",
    "            p35d = point_df.groupby(['lat','lon']).rolling(window=35,min_periods=1,on='time')[[air,rhum]].mean().reset_index().\\\n",
    "                  rename(columns={air:'p35d-t_level{}'.format(level),rhum:'p35d-h_level{}'.format(level)})\n",
    "            p35d['time'] = p35d['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            p7d = point_df.groupby(['lat','lon']).rolling(window=7,min_periods=1,on='time')[[air,rhum]].mean().reset_index().\\\n",
    "                  rename(columns={air:'p7d-t_level{}'.format(level),rhum:'p7d-h_level{}'.format(level)})\n",
    "            \n",
    "            p14d = point_df.groupby(['lat','lon']).rolling(window=14,min_periods=1,on='time')[[air,rhum]].mean().reset_index().\\\n",
    "                  rename(columns={air:'p14d-t_level{}'.format(level),rhum:'p14d-h_level{}'.format(level)})\n",
    "            p14d['time'] = p14d['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            #point_df_diff = point_df.groupby(['lat','lon'])[[air,rhum]].diff().fillna(0).reset_index().\\\n",
    "            #             rename(columns={air:'t_prev_diff_level{}'.format(level),rhum:'h_prev_diff_level{}'.format(level)})\n",
    "            #point_df_diff = point_df_diff.merge(point_df[['time','lat','lon']],left_index=True,right_index=True)\n",
    "            #point_df_diff['time'] = point_df_diff['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            lat_lon_df = point_df[point_df['time'] >= datetime.date(2019,11,1)].copy()\n",
    "            lat_lon_df.drop_duplicates(subset=['lat','lon'],keep='first',inplace=True)\n",
    "            lat_lon_df.rename(columns={'temperature_level{}'.format(level):'first_temperature_level{}'.format(level),\n",
    "                                      'humidity_level{}'.format(level):'first_humidity_level{}'.format(level)},inplace=True)\n",
    "            lat_lon_df['time'] = lat_lon_df['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            lat_lon_df_2 = point_df[point_df['time'] >= datetime.date(2019,9,1)].copy()\n",
    "            lat_lon_df_2.drop_duplicates(subset=['lat','lon'],keep='first',inplace=True)\n",
    "            lat_lon_df_2.rename(columns={'temperature_level{}'.format(level):'first_20190901_temperature_level{}'.format(level),\n",
    "                                      'humidity_level{}'.format(level):'first_20190901_humidity_level{}'.format(level)},inplace=True)\n",
    "            lat_lon_df_2['time'] = lat_lon_df_2['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            lat_lon_df_21 = p21d[p21d['time'] >= datetime.date(2019,11,1)].copy()\n",
    "            lat_lon_df_21.drop_duplicates(subset=['lat','lon'],keep='first',inplace=True)\n",
    "            lat_lon_df_21.rename(columns={'p21d-t_level{}'.format(level):'first_21d_temperature_level{}'.format(level),\n",
    "                                      'p21d-h_level{}'.format(level):'first_21d_humidity_level{}'.format(level)},inplace=True)\n",
    "            lat_lon_df_21['time'] = lat_lon_df_21['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            lat_lon_df_7 = p7d[p7d['time'] >= datetime.date(2019,10,1)].copy()\n",
    "            lat_lon_df_7.drop_duplicates(subset=['lat','lon'],keep='first',inplace=True)\n",
    "            lat_lon_df_7.rename(columns={'p7d-t_level{}'.format(level):'first_7d_temperature_level{}'.format(level),\n",
    "                                      'p7d-h_level{}'.format(level):'first_7d_humidity_level{}'.format(level)},inplace=True)\n",
    "            lat_lon_df_7['time'] = lat_lon_df_7['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            p21d['time'] = p21d['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            p7d['time'] = p7d['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            point_df['time'] = point_df['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            features_df = features_df.merge(p14d[['lat','lon','time','p14d-t_level{}'.format(level),'p14d-h_level{}'.format(level)]],\n",
    "                                on=['lat','lon','time'])\n",
    "            features_df = features_df.merge(p7d[['lat','lon','time','p7d-t_level{}'.format(level),'p7d-h_level{}'.format(level)]],\n",
    "                                on=['lat','lon','time'])\n",
    "            features_df = features_df.merge(p21d[['lat','lon','time','p21d-t_level{}'.format(level),'p21d-h_level{}'.format(level)]],\n",
    "                                on=['lat','lon','time'])\n",
    "            features_df = features_df.merge(p35d[['lat','lon','time','p35d-t_level{}'.format(level),'p35d-h_level{}'.format(level)]],\n",
    "                                on=['lat','lon','time'])\n",
    "            features_df = features_df.merge(point_df[['lat','lon','time','temperature_level{}'.format(level),'humidity_level{}'.format(level)]],\n",
    "                                on=['lat','lon','time'])\n",
    "            #features_df = features_df.merge(point_df_diff[['lat','lon','time','t_prev_diff_level{}'.format(level),\n",
    "            #                            'h_prev_diff_level{}'.format(level)]],on=['lat','lon','time'])\n",
    "            \n",
    "            features_df = features_df.merge(lat_lon_df_2[['lat','lon','first_20190901_temperature_level{}'.format(level),\n",
    "                                            'first_20190901_humidity_level{}'.format(level)]],on=['lat','lon'])\n",
    "            features_df = features_df.merge(lat_lon_df[['lat','lon','first_temperature_level{}'.format(level),\n",
    "                                            'first_humidity_level{}'.format(level)]],on=['lat','lon'])\n",
    "            features_df = features_df.merge(lat_lon_df_21[['lat','lon','first_21d_temperature_level{}'.format(level),\n",
    "                                            'first_21d_humidity_level{}'.format(level)]],on=['lat','lon'])\n",
    "            features_df = features_df.merge(lat_lon_df_7[['lat','lon','first_7d_temperature_level{}'.format(level),\n",
    "                                            'first_7d_humidity_level{}'.format(level)]],on=['lat','lon'])\n",
    "            features_df['p14d_diff-t_level{}'.format(level)] = (features_df['temperature_level{}'.format(level)] - \\\n",
    "                                               features_df['p14d-t_level{}'.format(level)])\n",
    "            features_df['p14d_diff-h_level{}'.format(level)] = (features_df['humidity_level{}'.format(level)] - \\\n",
    "                                               features_df['p14d-h_level{}'.format(level)])\n",
    "            #features_df['p35d_diff-t_level{}'.format(level)] = (features_df['temperature_level{}'.format(level)] - \\\n",
    "             #                                  features_df['p35d-t_level{}'.format(level)])\n",
    "            #features_df['p35d_diff-h_level{}'.format(level)] = (features_df['humidity_level{}'.format(level)] - \\\n",
    "             #                                  features_df['p35d-h_level{}'.format(level)])\n",
    "            \n",
    "            for nearest in [2,3,4]:\n",
    "                features_df = features_df.merge(point_df[['lat','lon','time','temperature_level{}'.format(level),'humidity_level{}'.format(level)]].\\\n",
    "                                              rename(columns={'temperature_level{}'.format(level):'temperature_near_{}_level_{}'.format(nearest,level),\n",
    "                                                              'humidity_level{}'.format(level):'humidity_near_{}_level_{}'.format(nearest,level),\n",
    "                                                             'lat':'lat_near_{}'.format(nearest),\n",
    "                                                             'lon':'lon_near_{}'.format(nearest)}),\\\n",
    "                                on=['lat_near_{}'.format(nearest),'lon_near_{}'.format(nearest),'time'])\n",
    "                \n",
    "                \n",
    "                features_df = features_df.merge(lat_lon_df[['lat','lon','first_temperature_level{}'.format(level),\n",
    "                                                          'first_humidity_level{}'.format(level)]].\\\n",
    "                    rename(columns={'first_temperature_level{}'.format(level):'first_temperature_near_{}_level_{}'.format(nearest,level),\n",
    "                    'first_humidity_level{}'.format(level):'first_humidity_near_{}_level_{}'.format(nearest,level),\n",
    "                    'lat':'lat_near_{}'.format(nearest),\n",
    "                    'lon':'lon_near_{}'.format(nearest)}),\\\n",
    "                                on=['lat_near_{}'.format(nearest),'lon_near_{}'.format(nearest)]) \n",
    "                features_df['distance_{}'.format(nearest)] = features_df[['lon_near_{}'.format(nearest),'lat_near_{}'.format(nearest),'longitude','latitude']]\\\n",
    "                      .apply(lambda x: calc_distance(x['lon_near_{}'.format(nearest)],x['lat_near_{}'.format(nearest)],x['longitude'],x['latitude']),axis=1)\n",
    "            \n",
    "                \n",
    "            \n",
    "            features_df['harmonic_mean-t_level{}'.format(level)] = (4/((1/features_df['temperature_level{}'.format(level)]) +\\\n",
    "                                                           (1/features_df['temperature_near_2_level_{}'.format(level)]) +\\\n",
    "                                                           (1/features_df['temperature_near_3_level_{}'.format(level)]) +\\\n",
    "                                                           (1/features_df['temperature_near_4_level_{}'.format(level)])))\n",
    "            \n",
    "            features_df['harmonic_mean-h_level{}'.format(level)] = (4/((1/features_df['humidity_level{}'.format(level)]) +\\\n",
    "                                                           (1/features_df['humidity_near_2_level_{}'.format(level)]) +\\\n",
    "                                                           (1/features_df['humidity_near_3_level_{}'.format(level)]) +\\\n",
    "                                                           (1/features_df['humidity_near_4_level_{}'.format(level)])))\n",
    "\n",
    "            features_df['dist_weighted_mean-t_level{}'.format(level)] = (features_df['temperature_level{}'.format(level)] *\\\n",
    "                                                           features_df['distance_from_nearest'] +\\\n",
    "                                                           features_df['temperature_near_2_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_2'] +\\\n",
    "                                                           features_df['temperature_near_3_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_3']+\\\n",
    "                                                           features_df['temperature_near_4_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_4'])/(features_df['distance_from_nearest'] + \\\n",
    "                                                    features_df['distance_2']+features_df['distance_3']+features_df['distance_4'])\n",
    "            \n",
    "            features_df['dist_weighted_mean-h_level{}'.format(level)] = (features_df['humidity_level{}'.format(level)] *\\\n",
    "                                                           features_df['distance_from_nearest'] +\\\n",
    "                                                           features_df['humidity_near_2_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_2'] +\\\n",
    "                                                           features_df['humidity_near_3_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_3']+\\\n",
    "                                                           features_df['humidity_near_4_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_4'])/(features_df['distance_from_nearest'] + \\\n",
    "                                                    features_df['distance_2']+features_df['distance_3']+features_df['distance_4'])\n",
    "            \n",
    "            features_df['mean-t_level{}'.format(level)] = (features_df['temperature_level{}'.format(level)] +\\\n",
    "                                                           features_df['temperature_near_2_level_{}'.format(level)] +\\\n",
    "                                                           features_df['temperature_near_3_level_{}'.format(level)] +\\\n",
    "                                                           features_df['temperature_near_4_level_{}'.format(level)])/4\n",
    "            \n",
    "            features_df['mean-h_level{}'.format(level)] = (features_df['humidity_level{}'.format(level)] +\\\n",
    "                                                           features_df['humidity_near_2_level_{}'.format(level)] +\\\n",
    "                                                           features_df['humidity_near_3_level_{}'.format(level)] +\\\n",
    "                                                           features_df['humidity_near_4_level_{}'.format(level)])/4\n",
    "            \n",
    "            \n",
    "            features_df['first_mean-t_level{}'.format(level)] = (features_df['first_temperature_level{}'.format(level)] +\\\n",
    "                                                           features_df['first_temperature_near_2_level_{}'.format(level)] +\\\n",
    "                                                           features_df['first_temperature_near_3_level_{}'.format(level)] +\\\n",
    "                                                           features_df['first_temperature_near_4_level_{}'.format(level)])/4\n",
    "            \n",
    "            features_df['first_mean-h_level{}'.format(level)] = (features_df['first_humidity_level{}'.format(level)] +\\\n",
    "                                                           features_df['first_humidity_near_2_level_{}'.format(level)] +\\\n",
    "                                                           features_df['first_humidity_near_3_level_{}'.format(level)] +\\\n",
    "                                                           features_df['first_humidity_near_4_level_{}'.format(level)])/4\n",
    "            \n",
    "            features_df['dist_weighted_first_mean-t_level{}'.format(level)] = (features_df['first_temperature_level{}'.format(level)] *\\\n",
    "                                                           features_df['distance_from_nearest'] +\\\n",
    "                                                           features_df['first_temperature_near_2_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_2'] +\\\n",
    "                                                           features_df['first_temperature_near_3_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_3'] +\\\n",
    "                                                           features_df['first_temperature_near_4_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_4'])/(features_df['distance_from_nearest'] + \\\n",
    "                                                    features_df['distance_2']+features_df['distance_3']+features_df['distance_4'])\n",
    "            \n",
    "            features_df['dist_weighted_irst_mean-h_level{}'.format(level)] = (features_df['first_humidity_level{}'.format(level)] *\\\n",
    "                                                           features_df['distance_from_nearest'] +\\\n",
    "                                                           features_df['first_humidity_near_2_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_2'] +\\\n",
    "                                                           features_df['first_humidity_near_3_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_3'] +\\\n",
    "                                                           features_df['first_humidity_near_4_level_{}'.format(level)] *\\\n",
    "                                                           features_df['distance_4'])/(features_df['distance_from_nearest'] + \\\n",
    "                                                    features_df['distance_2']+features_df['distance_3']+features_df['distance_4'])\n",
    "            \n",
    "            features_df['diff_mean-t_level{}'.format(level)] = features_df['temperature_level{}'.format(level)] - features_df['mean-t_level{}'.format(level)]\n",
    "            features_df['diff_first_mean-t_level{}'.format(level)] = features_df['temperature_level{}'.format(level)] - features_df['first_mean-t_level{}'.format(level)]\n",
    "            features_df['diff_first_mean-first_t_level{}'.format(level)] = features_df['first_temperature_level{}'.format(level)] - features_df['first_mean-t_level{}'.format(level)]\n",
    "            features_df['diff_first-t_level{}'.format(level)] = features_df['temperature_level{}'.format(level)] - features_df['first_temperature_level{}'.format(level)]\n",
    "            features_df['diff_mean-h_level{}'.format(level)] = features_df['humidity_level{}'.format(level)] - features_df['mean-h_level{}'.format(level)]\n",
    "            features_df['diff_first_mean-h_level{}'.format(level)] = features_df['humidity_level{}'.format(level)] - features_df['first_mean-h_level{}'.format(level)]\n",
    "            features_df['diff_first-h_level{}'.format(level)] = features_df['humidity_level{}'.format(level)] - features_df['first_humidity_level{}'.format(level)]\n",
    "            features_df['diff_first_mean-first_h_level{}'.format(level)] = features_df['first_humidity_level{}'.format(level)] - features_df['first_mean-h_level{}'.format(level)]\n",
    "            #features_df['diff_first-h-t_level{}'.format(level)] = features_df['diff_first-h_level{}'.format(level)] * features_df['diff_first-t_level{}'.format(level)]\n",
    "            #features_df['h-t_level{}'.format(level)] = features_df['temperature_level{}'.format(level)] * features_df['humidity_level{}'.format(level)]\n",
    "        for level_1,level_2 in [(1000,700),(100,50),(1000,50),(1000,300),(70,50),(150,50)]:\n",
    "            features_df['temperature_level_{}_{}_diff'.format(level_1,level_2)] = features_df['temperature_level{}'.format(level_1)] -  features_df['temperature_level{}'.format(level_2)]\n",
    "            features_df['p14d-t_level_{}_{}_diff'.format(level_1,level_2)] = features_df['p14d-t_level{}'.format(level_1)] -  features_df['p14d-t_level{}'.format(level_2)]\n",
    "            features_df['first_mean-t_level_{}_{}_diff'.format(level_1,level_2)] = features_df['first_mean-t_level{}'.format(level_1)] -  features_df['first_mean-t_level{}'.format(level_2)]\n",
    "            features_df['first-t_level_{}_{}_diff'.format(level_1,level_2)] = features_df['first_temperature_level{}'.format(level_1)] -  features_df['first_temperature_level{}'.format(level_2)]\n",
    "        for level_1,level_2 in [(1000,700),(500,300),(1000,300),(700,300)]:\n",
    "            features_df['humidity_level_{}_{}_diff'.format(level_1,level_2)] = features_df['humidity_level{}'.format(level_1)] -  features_df['humidity_level{}'.format(level_2)]\n",
    "            features_df['p14d-h_level_{}_{}_diff'.format(level_1,level_2)] = features_df['p14d-h_level{}'.format(level_1)] -  features_df['p14d-h_level{}'.format(level_2)]\n",
    "            features_df['first_mean-h_level_{}_{}_diff'.format(level_1,level_2)] = features_df['first_mean-h_level{}'.format(level_1)] -  features_df['first_mean-h_level{}'.format(level_2)]\n",
    "            features_df['first-h_level_{}_{}_diff'.format(level_1,level_2)] = features_df['first_humidity_level{}'.format(level_1)] -  features_df['first_humidity_level{}'.format(level_2)]\n",
    "        return features_df.drop(['lat_near_2','lon_near_2','lat_near_3','lon_near_3','lat_near_4','lon_near_4',\n",
    "                                 'time','latitude','longitude',\n",
    "                            'temperature_near_2_level_{}'.format(level),'temperature_near_3_level_{}'.format(level),\n",
    "                                'temperature_near_4_level_{}'.format(level),'humidity_near_3_level_{}'.format(level),\n",
    "                                'humidity_near_3_level_{}'.format(level),'humidity_near_3_level_{}'.format(level),\n",
    "                                'first_temperature_near_2_level_{}'.format(level),'first_temperature_near_3_level_{}'.format(level),\n",
    "                                'first_temperature_near_4_level_{}'.format(level),'first_humidity_near_3_level_{}'.format(level),\n",
    "                                'first_humidity_near_3_level_{}'.format(level),'first_humidity_near_3_level_{}'.format(level)],\n",
    "                                 axis=1,errors='ignore')   \n",
    "                       \n",
    "    \n",
    "    def extract_winds_features(self,df):   \n",
    "        features_df = df[['fire_id','longitude','latitude','lat','lon','lat_near_2','lon_near_2','lat_near_3','lon_near_3','lat_near_4','lon_near_4',\n",
    "                                  'time']].copy()\n",
    "        \n",
    "        for level in self.base_levels:\n",
    "            point_df = self.ncep_data.sel(\n",
    "                level=level,\n",
    "            ).to_dataframe().reset_index().rename(columns={'uwnd':'uwind_level{}'.format(level),\n",
    "                                                          'vwnd':'vwind_level{}'.format(level)})\n",
    "            uwnd = 'uwind_level{}'.format(level)\n",
    "            vwnd = 'vwind_level{}'.format(level)\n",
    "            \n",
    "            if level == self.base_levels[0]:\n",
    "                print(point_df['time'].max(),point_df['time'].min())\n",
    "            \n",
    "            p21d = point_df.groupby(['lat','lon']).rolling(window=21,min_periods=1,on='time')[[uwnd,vwnd]].mean().reset_index().\\\n",
    "                  rename(columns={uwnd:'p21d-uw_level{}'.format(level),vwnd:'p21d-vw_level{}'.format(level)})\n",
    "            \n",
    "            p35d = point_df.groupby(['lat','lon']).rolling(window=35,min_periods=1,on='time')[[uwnd]].mean().reset_index().\\\n",
    "                  rename(columns={uwnd:'p35d-uw_level{}'.format(level)})\n",
    "            p35d['time'] = p35d['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            #p50d = point_df.groupby(['lat','lon']).rolling(window=50,min_periods=1,on='time')[[uwnd]].mean().reset_index().\\\n",
    "            #      rename(columns={uwnd:'p50d-uw_level{}'.format(level)})\n",
    "            #p50d['time'] = p50d['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            p7d = point_df.groupby(['lat','lon']).rolling(window=7,min_periods=1,on='time')[[uwnd,vwnd]].mean().reset_index().\\\n",
    "                  rename(columns={uwnd:'p7d-uw_level{}'.format(level),vwnd:'p7d-vw_level{}'.format(level)})\n",
    "            \n",
    "            p14d = point_df.groupby(['lat','lon']).rolling(window=14,min_periods=1,on='time')[[uwnd]].mean().reset_index().\\\n",
    "                  rename(columns={uwnd:'p14d-uw_level{}'.format(level)})\n",
    "            p14d['time'] = p14d['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            point_df_diff = point_df.groupby(['lat','lon'])[[uwnd]].diff().fillna(0).reset_index().\\\n",
    "                         rename(columns={uwnd:'uw_prev_diff_level{}'.format(level)})\n",
    "            point_df_diff = point_df_diff.merge(point_df[['time','lat','lon']],left_index=True,right_index=True)\n",
    "            point_df_diff['time'] = point_df_diff['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            lat_lon_df = point_df[point_df['time'] >= datetime.date(2019,11,1)].copy()\n",
    "            lat_lon_df.drop_duplicates(subset=['lat','lon'],keep='first',inplace=True)\n",
    "            lat_lon_df.rename(columns={'uwind_level{}'.format(level):'first_uwind_level{}'.format(level),\n",
    "                                      'vwind_level{}'.format(level):'first_vwind_level{}'.format(level)},inplace=True)\n",
    "            lat_lon_df['time'] = lat_lon_df['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            lat_lon_df_2 = point_df[point_df['time'] >= datetime.date(2019,9,1)].copy()\n",
    "            lat_lon_df_2.drop_duplicates(subset=['lat','lon'],keep='first',inplace=True)\n",
    "            lat_lon_df_2.rename(columns={'uwind_level{}'.format(level):'first_20190901_uwind_level{}'.format(level),\n",
    "                                      'vwind_level{}'.format(level):'first_20190901_vwind_level{}'.format(level)},inplace=True)\n",
    "            lat_lon_df_2['time'] = lat_lon_df_2['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            lat_lon_df_21 = p21d[p21d['time'] >= datetime.date(2019,11,1)].copy()\n",
    "            lat_lon_df_21.drop_duplicates(subset=['lat','lon'],keep='first',inplace=True)\n",
    "            lat_lon_df_21.rename(columns={'p21d-uw_level{}'.format(level):'first_21d_uwind_level{}'.format(level),\n",
    "                                      'p21d-vw_level{}'.format(level):'first_21d_vwind_level{}'.format(level)},inplace=True)\n",
    "            lat_lon_df_21['time'] = lat_lon_df_21['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            lat_lon_df_7 = p7d[p7d['time'] >= datetime.date(2019,10,1)].copy()\n",
    "            lat_lon_df_7.drop_duplicates(subset=['lat','lon'],keep='first',inplace=True)\n",
    "            lat_lon_df_7.rename(columns={'p7d-uw_level{}'.format(level):'first_7d_uwind_level{}'.format(level),\n",
    "                                      'p7d-vw_level{}'.format(level):'first_7d_vwind_level{}'.format(level)},inplace=True)\n",
    "            lat_lon_df_7['time'] = lat_lon_df_7['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            point_df['time'] = point_df['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            p21d['time'] = p21d['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            p7d['time'] = p7d['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "                \n",
    "\n",
    "            features_df = features_df.merge(p14d[['lat','lon','time','p14d-uw_level{}'.format(level)]],\n",
    "                                on=['lat','lon','time'])\n",
    "            features_df = features_df.merge(p7d[['lat','lon','time','p7d-uw_level{}'.format(level)]],\n",
    "                                on=['lat','lon','time'])\n",
    "            features_df = features_df.merge(p21d[['lat','lon','time','p21d-uw_level{}'.format(level)]],\n",
    "                                on=['lat','lon','time'])\n",
    "            features_df = features_df.merge(p35d[['lat','lon','time','p35d-uw_level{}'.format(level)]],\n",
    "                                on=['lat','lon','time'])\n",
    "            features_df = features_df.merge(point_df[['lat','lon','time','uwind_level{}'.format(level)]],\n",
    "                                on=['lat','lon','time'])\n",
    "            #features_df = features_df.merge(point_df_diff[['lat','lon','time',\n",
    "            #                        'uw_prev_diff_level{}'.format(level)]],on=['lat','lon','time'])\n",
    "            \n",
    "            features_df = features_df.merge(lat_lon_df[['lat','lon','first_uwind_level{}'.format(level),\n",
    "                                                       'first_vwind_level{}'.format(level)]],\n",
    "                                            on=['lat','lon'])\n",
    "            \n",
    "            features_df['p14d_diff-uw_level{}'.format(level)] = (features_df['uwind_level{}'.format(level)] - \\\n",
    "                                               features_df['p14d-uw_level{}'.format(level)])\n",
    "            #features_df['p35d_diff-uw_level{}'.format(level)] = (features_df['uwind_level{}'.format(level)] - \\\n",
    "            #                                   features_df['p35d-uw_level{}'.format(level)])\n",
    "\n",
    "            for nearest in [2,3,4]:\n",
    "                features_df = features_df.merge(point_df[['lat','lon','time','uwind_level{}'.format(level)]].\\\n",
    "                                              rename(columns={'uwind_level{}'.format(level):'uwind_near_{}_level_{}'.format(nearest,level),\n",
    "                                                             'lat':'lat_near_{}'.format(nearest),\n",
    "                                                             'lon':'lon_near_{}'.format(nearest)}),\n",
    "                                on=['lat_near_{}'.format(nearest),'lon_near_{}'.format(nearest),'time'])\n",
    "                \n",
    "                features_df = features_df.merge(lat_lon_df[['lat','lon','first_uwind_level{}'.format(level),\n",
    "                                                           'first_vwind_level{}'.format(level)]].\\\n",
    "                    rename(columns={'first_uwind_level{}'.format(level):'first_uwind_near_{}_level_{}'.format(nearest,level),\n",
    "                                    'first_vwind_level{}'.format(level):'first_vwind_near_{}_level_{}'.format(nearest,level),\n",
    "                    'lat':'lat_near_{}'.format(nearest),\n",
    "                    'lon':'lon_near_{}'.format(nearest)}),\\\n",
    "                                on=['lat_near_{}'.format(nearest),'lon_near_{}'.format(nearest)]) \n",
    "\n",
    "              \n",
    "            \n",
    "            features_df['harmonic_mean-uw_level{}'.format(level)] = (4/((1/features_df['uwind_level{}'.format(level)]) +\\\n",
    "                                                           (1/features_df['uwind_near_2_level_{}'.format(level)]) +\\\n",
    "                                                           (1/features_df['uwind_near_3_level_{}'.format(level)]) +\\\n",
    "                                                           (1/features_df['uwind_near_4_level_{}'.format(level)])))\n",
    "            \n",
    "            #features_df['harmonic_mean-vw_level{}'.format(level)] = (4/((1/features_df['vwind_level{}'.format(level)]) +\\\n",
    "            #                                               (1/features_df['vwind_near_2_level_{}'.format(level)]) +\\\n",
    "            #                                               (1/features_df['vwind_near_3_level_{}'.format(level)]) +\\\n",
    "            #                                               (1/features_df['vwind_near_4_level_{}'.format(level)])))\n",
    "            \n",
    "            features_df['mean-uw_level{}'.format(level)] = (features_df['uwind_level{}'.format(level)] +\\\n",
    "                                                           features_df['uwind_near_2_level_{}'.format(level)] +\\\n",
    "                                                           features_df['uwind_near_3_level_{}'.format(level)] +\\\n",
    "                                                           features_df['uwind_near_4_level_{}'.format(level)])/4\n",
    "            \n",
    "            #features_df['mean-vw_level{}'.format(level)] = (features_df['vwind_level{}'.format(level)] +\\\n",
    "            #                                               features_df['vwind_near_2_level_{}'.format(level)] +\\\n",
    "            #                                               features_df['vwind_near_3_level_{}'.format(level)] +\\\n",
    "            #                                               features_df['vwind_near_4_level_{}'.format(level)])/4\n",
    "            \n",
    "            features_df['first_mean-uw_level{}'.format(level)] = (features_df['first_uwind_level{}'.format(level)] +\\\n",
    "                                                           features_df['first_uwind_near_2_level_{}'.format(level)] +\\\n",
    "                                                           features_df['first_uwind_near_3_level_{}'.format(level)] +\\\n",
    "                                                           features_df['first_uwind_near_4_level_{}'.format(level)])/4\n",
    "            features_df['first_mean-vw_level{}'.format(level)] = (features_df['first_vwind_level{}'.format(level)] +\\\n",
    "                                                           features_df['first_vwind_near_2_level_{}'.format(level)] +\\\n",
    "                                                           features_df['first_vwind_near_3_level_{}'.format(level)] +\\\n",
    "                                                           features_df['first_vwind_near_4_level_{}'.format(level)])/4\n",
    "            \n",
    "            features_df['first_wind_module_level{}'.format(level)] = (features_df['first_vwind_level{}'.format(level)] ** 2 + \\\n",
    "                                                               features_df['first_uwind_level{}'.format(level)] ** 2)**0.5\n",
    "            features_df['first_wind_direction_level{}'.format(level)] = features_df[['first_uwind_level{}'.format(level),\n",
    "                                                'first_vwind_level{}'.format(level),\n",
    "                                                'first_wind_module_level{}'.format(level)]].\\\n",
    "                                                 apply(lambda x:self.wind_direction(x['first_uwind_level{}'.format(level)],\n",
    "                                                                x['first_vwind_level{}'.format(level)],\n",
    "                                                                        x['first_wind_module_level{}'.format(level)]),axis=1)\n",
    "            \n",
    "            features_df['diff_mean-uw_level{}'.format(level)] = features_df['uwind_level{}'.format(level)] - features_df['mean-uw_level{}'.format(level)]\n",
    "            features_df['diff_first_mean-uw_level{}'.format(level)] = features_df['uwind_level{}'.format(level)] - features_df['first_mean-uw_level{}'.format(level)]\n",
    "            features_df['diff_first-uw_level{}'.format(level)] = features_df['uwind_level{}'.format(level)] - features_df['first_uwind_level{}'.format(level)]\n",
    "            features_df['diff_first_mean-first_uw_level{}'.format(level)] = features_df['first_uwind_level{}'.format(level)] - features_df['first_mean-uw_level{}'.format(level)]\n",
    "            features_df['diff_first_mean-first_vw_level{}'.format(level)] = features_df['first_vwind_level{}'.format(level)] - features_df['first_mean-vw_level{}'.format(level)]\n",
    "        for level_1,level_2 in [(1000,700),(100,50),(1000,50),(1000,300),(70,50),(150,50)]:   \n",
    "            features_df['uwind_level_{}_{}_diff'.format(level_1,level_2)] = features_df['uwind_level{}'.format(level_1)] -  features_df['uwind_level{}'.format(level_2)]\n",
    "            features_df['p14d-uw_level_{}_{}_diff'.format(level_1,level_2)] = features_df['p14d-uw_level{}'.format(level_1)] -  features_df['p14d-uw_level{}'.format(level_2)]\n",
    "            features_df['first_mean-uw_level_{}_{}_diff'.format(level_1,level_2)] = features_df['first_mean-uw_level{}'.format(level_1)] -  features_df['first_mean-uw_level{}'.format(level_2)]\n",
    "            features_df['first_mean-vw_level_{}_{}_diff'.format(level_1,level_2)] = features_df['first_mean-vw_level{}'.format(level_1)] -  features_df['first_mean-vw_level{}'.format(level_2)]\n",
    "            features_df['first-uw_level_{}_{}_diff'.format(level_1,level_2)] = features_df['first_uwind_level{}'.format(level_1)] -  features_df['first_uwind_level{}'.format(level_2)]\n",
    "            features_df['first-vw_level_{}_{}_diff'.format(level_1,level_2)] = features_df['first_vwind_level{}'.format(level_1)] -  features_df['first_vwind_level{}'.format(level_2)]\n",
    "            #features_df['first_mean-wind_module_level_{}_{}_diff'.format(level_1,level_2)] = features_df['first_wind_module_level{}'.format(level_1)] -  features_df['first_wind_module_level{}'.format(level_2)]\n",
    "        \n",
    "        return features_df.drop(['lat','lon','lat_near_2','lon_near_2','lat_near_3','lon_near_3','lat_near_4','lon_near_4','time',\n",
    "                                'bearing_from_nearest','latitude','longitude',\n",
    "                                'uwind_near_2_level_{}'.format(level),'uwind_near_3_level_{}'.format(level),\n",
    "                                'uwind_near_4_level_{}'.format(level),'vwind_near_2_level_{}'.format(level),\n",
    "                                'vwind_near_3_level_{}'.format(level),'vwind_near_4_level_{}'.format(level),\n",
    "                                'first_uwind_near_2_level_{}'.format(level),'first_uwind_near_3_level_{}'.format(level),\n",
    "                                'first_uwind_near_4_level_{}'.format(level),'first_vwind_near_2_level_{}'.format(level),\n",
    "                                'first_vwind_near_3_level_{}'.format(level),'first_vwind_near_4_level_{}'.format(level)],\n",
    "                                axis=1,errors='ignore')\n",
    "    \n",
    "    def make_winds_features(self,df):\n",
    "        start = time.time()\n",
    "        print('Reading winds dataset...')\n",
    "        self.read_winds_set()\n",
    "        print(\"Reading dataset done! Time {:.2f}\".format(time.time()-start))\n",
    "        start = time.time()\n",
    "        print('Making wind features...')\n",
    "        winds_features = self.extract_winds_features(df)\n",
    "        print('Making wind features done! Time {:.2f}'.format(time.time()-start))\n",
    "        print(pd.DataFrame(winds_features).shape)\n",
    "        return pd.DataFrame(winds_features).set_index('fire_id')\n",
    "    \n",
    "    def read_winds_set(self):       \n",
    "        ncep_data = []\n",
    "        for var in ['uwnd','vwnd']:\n",
    "            print(var)\n",
    "            for year in range(2012,2020,1):\n",
    "                dataset_filename = '{}.{}.nc'.format(var, year)\n",
    "                array = xarray.open_dataset(dataset_filename)\n",
    "                array = array.where((array.lat >= self.lat_min) &\n",
    "                      (array.lat <= self.lat_max) & \n",
    "                      (array.lon <= self.lon_max) & \n",
    "                      (array.lon >= self.lon_min) & \n",
    "                     (array.level.isin(self.base_levels)),drop=True)\n",
    "                ncep_data.append(array)\n",
    "            full_array = xarray.merge(ncep_data)\n",
    "        self.ncep_data = full_array\n",
    "        \n",
    "    def make_all_features(self,df):\n",
    "        if df['date'].dtype== 'O':\n",
    "            df['date'] = pd.to_datetime(df['date'])       \n",
    "            \n",
    "        df['lon'] = self.lat_lons[self.kd_tree.query(df[['longitude','latitude']],1,return_distance=False)[:,0]][:,0]\n",
    "        df['lat'] = self.lat_lons[self.kd_tree.query(df[['longitude','latitude']],1,return_distance=False)[:,0]][:,1]\n",
    "        df['time'] = df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        \n",
    "        for nearest in [2,3,4]:\n",
    "            df['lon_near_{}'.format(nearest)] = self.lat_lons[self.kd_tree.query(df[['longitude','latitude']],4,return_distance=False)[:,nearest-1]][:,0]\n",
    "            df['lat_near_{}'.format(nearest)] = self.lat_lons[self.kd_tree.query(df[['longitude','latitude']],4,return_distance=False)[:,nearest-1]][:,1]  \n",
    "   \n",
    "        features = self.make_raw_features(df)\n",
    "        print(features.shape)\n",
    "        features = features.merge(self.make_winds_features(df),left_index=True,right_index=True)\n",
    "        print(features.shape)\n",
    "        features = features.merge(self.make_temp_rhum_features(df),left_index=True,right_index=True)\n",
    "        return features.drop(['lat_near_2','lat_near_3','lat_near_4',\n",
    "                              'lon_near_2','lon_near_3','lon_near_4'],errors='ignore')\n",
    "    \n",
    "    def make_labels(self,df):\n",
    "        return df.set_index('fire_id')['fire_type']\n",
    "    \n",
    "    def make_train_set(self,df):\n",
    "        X = self.make_all_features(df)\n",
    "        y = self.make_labels(df)\n",
    "        return X,y\n",
    "    \n",
    "    \n",
    "def time_series_features(full_df,train,test):\n",
    "    '''\n",
    "        full_df - весь датасет\n",
    "        train - обучающая выборка\n",
    "        test - тестовая выборка\n",
    "        \n",
    "        алгоритм такой:\n",
    "          1) кластеризация делаем по всему full_df;\n",
    "          2) количество, площади и т.д. считаем только по train\n",
    "          3) получившиеся фичи из train энкодим в test по метке кластера\n",
    "    '''\n",
    "    dbs = DBSCAN(eps=0.035,min_samples=1,n_jobs=4)\n",
    "    full_df.sort_index(inplace=True)\n",
    "    train.sort_index(inplace=True)\n",
    "    test.sort_index(inplace=True)\n",
    "    full_df['label'] = dbs.fit_predict(X[['latitude','longitude']])\n",
    "    train['label'] = full_df.loc[train.index,'label']\n",
    "    test['label'] = full_df.loc[test.index,'label']\n",
    "    train['count_035'] = train.groupby('label')['latitude'].transform('count')\n",
    "    train['dist_035_latitude_mean'] = train.groupby('label')['latitude'].transform('mean')\n",
    "    train['dist_035_longitude_mean'] = train.groupby('label')['longitude'].transform('mean')\n",
    "    train['dist_035_square'] = (train.groupby('label')['latitude'].transform('max') - \\\n",
    "                                   train.groupby('label')['latitude'].transform('min'))*\\\n",
    "                                (train.groupby('label')['longitude'].transform('max') - \\\n",
    "                                   train.groupby('label')['longitude'].transform('min'))\n",
    "    for col in ['dist_035_latitude_mean','dist_035_longitude_mean','dist_035_square','count_035']:\n",
    "        map_dict = train.drop_duplicates('label').set_index('label')[col]\n",
    "        test[col] = full_df.loc[test.index,'label'].map(map_dict)\n",
    "        nan_ind = test[test[col].isnull()].index\n",
    "        if col == 'count_035':\n",
    "            test.loc[nan_ind,col] = 0\n",
    "        test.loc[nan_ind,col] = -1\n",
    "\n",
    "    train['distance_from_035'] = train[['dist_035_longitude_mean','dist_035_latitude_mean','longitude','latitude']]\\\n",
    "                                .apply(lambda x: calc_distance(x['dist_035_longitude_mean'],x['dist_035_latitude_mean'],x['longitude'],x['latitude']),axis=1)\n",
    "    test['distance_from_035'] = test[['dist_035_longitude_mean','dist_035_latitude_mean','longitude','latitude']]\\\n",
    "                                .apply(lambda x: calc_distance(x['dist_035_longitude_mean'],x['dist_035_latitude_mean'],x['longitude'],x['latitude']),axis=1)\n",
    "    \n",
    "    dbs = DBSCAN(eps=0.15,min_samples=1,n_jobs=4)\n",
    "    full_df['label'] = dbs.fit_predict(X[['latitude','longitude']])\n",
    "    train['label'] = full_df.loc[train.index,'label']\n",
    "    test['label'] = full_df.loc[test.index,'label']\n",
    "    train['count_15'] = train.groupby('label')['latitude'].transform('count')\n",
    "    train['dist_15_square'] = (train.groupby('label')['latitude'].transform('max') - \\\n",
    "                                   train.groupby('label')['latitude'].transform('min'))*\\\n",
    "                                (train.groupby('label')['longitude'].transform('max') - \\\n",
    "                                   train.groupby('label')['longitude'].transform('min'))\n",
    "    for col in ['dist_15_square','count_15']:\n",
    "        map_dict = train.drop_duplicates('label').set_index('label')[col]\n",
    "        test[col] = full_df.loc[test.index,'label'].map(map_dict)\n",
    "        nan_ind = test[test[col].isnull()].index\n",
    "        if col == 'count_15':\n",
    "            test.loc[nan_ind,col] = 0\n",
    "        test.loc[nan_ind,col] = -1\n",
    "    train['square_ratio'] = (train['dist_035_square']/train['dist_15_square']).fillna(0)\n",
    "    test['square_ratio'] = (test['dist_035_square']/test['dist_15_square']).fillna(0)\n",
    "    train['count_035_divide_count_15'] = (train['count_035']/train['count_15']).fillna(0)\n",
    "    test['count_035_divide_count_15'] = (test['count_035']/test['count_15']).fillna(0)\n",
    "\n",
    "    #train['latitude_big_round'] = train['latitude'].round(0)\n",
    "    #train['longitude_big_round'] = train['longitude'].round(0)\n",
    "    #test['latitude_big_round'] = test['latitude'].round(0)\n",
    "    #test['longitude_big_round'] = test['longitude'].round(0)\n",
    "    return pd.concat([train,test]).drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_tree = KDTree(df_train[['longitude','latitude']])\n",
    "kd_time_tree = KDTree(df_train[['longitude','latitude','timestamp']])\n",
    "\n",
    "date_list = []\n",
    "start_date = datetime.date(2011,10,1)\n",
    "while start_date < datetime.date(2019,5,1):\n",
    "    date_list.append(start_date)\n",
    "    start_date += relativedelta(days=1)\n",
    "min_date_dict = pd.Series(np.nan,index=pd.to_datetime(date_list))\n",
    "max_date_dict = min_date_dict.copy()\n",
    "min_date_dict = df_train.drop_duplicates('date',keep='first').reset_index().set_index('date')['fire_id'].\\\n",
    "                       combine(min_date_dict,max).fillna(method='bfill').fillna(-1).astype(int).to_dict()\n",
    "max_date_dict = df_train.drop_duplicates('date',keep='first').reset_index().set_index('date')['fire_id'].\\\n",
    "                       combine(max_date_dict,max).fillna(method='ffill').fillna(-1).astype(int).to_dict()\n",
    "def return_nearest_previous_coord(row):\n",
    "    indices,distances = kd_tree.query_radius([[row['longitude'],row['latitude']]],0.05,\n",
    "                                    return_distance=True,sort_results=True)\n",
    "    indices = indices[0][1:]\n",
    "   #print(indices)\n",
    "    \n",
    "    count_3_years = 0\n",
    "    \n",
    "    count_last_year = 0\n",
    "    \n",
    "    lat_last_year_nearest = 0\n",
    "    lon_last_year_nearest = 0\n",
    "    distance_last_year_nearest = -1\n",
    "    bearing_last_year_nearest = -200\n",
    "    last_year_class = -1\n",
    "    \n",
    "    delta_week = relativedelta(days=7)\n",
    "    delta_month = relativedelta(days=14)\n",
    "    delta_year = relativedelta(years=1)\n",
    "    start_year = row['date'] - delta_year\n",
    "    condition = None\n",
    "    condition_month = None\n",
    "    condition_week = None\n",
    "    last_year_condition = None\n",
    "    last_year_condition_month = None\n",
    "    end_year = row['date'].replace(year=2019,day=1,month=1)\n",
    "    n = 0\n",
    "    #print(start_year)\n",
    "    while start_year >= datetime.date(2012,1,1):\n",
    "        if n >= 3:\n",
    "            break\n",
    "        min_date_month = start_year - delta_month\n",
    "        max_date_month = start_year + delta_month  \n",
    "        min_date_week = start_year - delta_week\n",
    "        max_date_week = start_year + delta_week   \n",
    "        try:\n",
    "            if condition is None:\n",
    "                #condition = ((indices <= max_date_dict[max_date]) & (indices >= min_date_dict[min_date]))\n",
    "                condition_month = ((indices <= max_date_dict[max_date_month]) & (indices >= min_date_dict[min_date_month]))\n",
    "                condition_week = ((indices <= max_date_dict[max_date_week]) & (indices >= min_date_dict[min_date_week]))\n",
    "                #last_year_condition = (indices <= max_date_dict[max_date]) & (indices >= min_date_dict[min_date])\n",
    "                last_year_condition_month = (indices <= max_date_dict[max_date_month]) & (indices >= min_date_dict[min_date_month])\n",
    "            else:\n",
    "                #condition |= ((indices <= max_date_dict[max_date]) & (indices >= min_date_dict[min_date]))\n",
    "                condition_month = ((indices <= max_date_dict[max_date_month]) & (indices >= min_date_dict[min_date_month]))\n",
    "                condition_week = ((indices <= max_date_dict[max_date_week]) & (indices >= min_date_dict[min_date_week]))\n",
    "        except:\n",
    "            print(max_date,min_date)\n",
    "            #print(condition)\n",
    "            raise\n",
    "        start_year -= delta_year\n",
    "        n += 1\n",
    "    #print(condition)\n",
    "    #print(last_year_condition)\n",
    "    #filtered_ind = np.where(last_year_condition)[0]\n",
    "    #good_ind = indices[filtered_ind]\n",
    "    #merge_dict = {'full_count_5km':len(indices)}\n",
    "    if len(indices) > 0:\n",
    "        all_condition = (indices <= max_date_dict[end_year]) \n",
    "        filtered_ind = np.where(all_condition)[0]\n",
    "        merge_dict = {'5km_count':len(filtered_ind)}\n",
    "    else:\n",
    "        merge_dict = {'5km_count':0}\n",
    "        \n",
    "    filtered_ind = np.where(last_year_condition_month)[0]\n",
    "    good_ind = indices[filtered_ind]\n",
    "    merge_dict.update({'last_3_years_count':len(good_ind)})\n",
    "    if len(good_ind) > 0:\n",
    "        lat_last_year_nearest = df_train.iloc[good_ind[0]]['latitude']\n",
    "        lon_last_year_nearest = df_train.iloc[good_ind[0]]['longitude']\n",
    "        distance_last_year_nearest = calc_distance(row['longitude'],row['latitude'],\n",
    "                                                   lon_last_year_nearest,lat_last_year_nearest)\n",
    "        bearing_last_year_nearest = calc_bearing(row['longitude'],row['latitude'],\n",
    "                                                  lon_last_year_nearest,lat_last_year_nearest)\n",
    "        \n",
    "    merge_dict.update({'diff_month_nearest_latitude':lat_last_year_nearest,\n",
    "                      'diff_month_nearest_longitude':lon_last_year_nearest,\n",
    "                      'diff_month_nearest_distance':distance_last_year_nearest,\n",
    "                      'diff_month_count':len(good_ind),\n",
    "                     })\n",
    "    \n",
    "    \n",
    "    filtered_ind = np.where(condition_month)[0]\n",
    "    good_ind = indices[filtered_ind]\n",
    "    merge_dict.update({'diff_3_years_month_count':len(good_ind)})\n",
    "    if len(good_ind) > 0:\n",
    "        lat_last_year_nearest = df_train.iloc[good_ind[0]]['latitude']\n",
    "        lon_last_year_nearest = df_train.iloc[good_ind[0]]['longitude']\n",
    "        distance_last_year_nearest = calc_distance(row['longitude'],row['latitude'],\n",
    "                                                   lon_last_year_nearest,lat_last_year_nearest)\n",
    "        bearing_last_year_nearest = calc_bearing(row['longitude'],row['latitude'],\n",
    "                                                  lon_last_year_nearest,lat_last_year_nearest)\n",
    "        \n",
    "    merge_dict.update({'diff_3_years_month_nearest_latitude':lat_last_year_nearest,\n",
    "                      'diff_3_years_month_nearest_longitude':lon_last_year_nearest,\n",
    "                      'diff_3_years_month_nearest_distance':distance_last_year_nearest,\n",
    "                      'diff_3_years_month_count':len(good_ind),\n",
    "                     })\n",
    "    \n",
    "    filtered_ind = np.where(condition_week)[0]\n",
    "    good_ind = indices[filtered_ind]\n",
    "    merge_dict.update({'last_year_diff_week_count':len(good_ind)})\n",
    "    \n",
    "    indices,distances = kd_time_tree.query_radius([[row['longitude'],row['latitude'],row['timestamp']]],\n",
    "                            0.1,return_distance=True,sort_results=True)\n",
    "    indices = indices[0][1:]\n",
    "    \n",
    "    count_same_day = 0\n",
    "    lat_same_day_nearest = 0\n",
    "    lon_same_day_nearest = 0\n",
    "    distance_same_day_nearest = -1\n",
    "    bearing_same_day_nearest = -200   \n",
    "    if len(indices) > 0:\n",
    "        lat_same_day_nearest = df_train.iloc[indices[0]]['latitude']\n",
    "        lon_same_day_nearest = df_train.iloc[indices[0]]['longitude']\n",
    "        distance_same_day_nearest = calc_distance(row['longitude'],row['latitude'],\n",
    "                                                   lon_same_day_nearest,lat_same_day_nearest)\n",
    "        bearing_same_day_nearest = calc_bearing(row['longitude'],row['latitude'],\n",
    "                                                  lon_same_day_nearest,lat_same_day_nearest)\n",
    "        count_same_day = len(indices)\n",
    "    merge_dict.update({'same_day_nearest_latitude_10km':lat_same_day_nearest,\n",
    "                      'same_day_nearest_longitude_10km':lon_same_day_nearest,\n",
    "                      'same_day_nearest_distance_10km':distance_same_day_nearest,\n",
    "                      'same_day_nearest_bearing_10km':bearing_same_day_nearest,\n",
    "                      'same_day_count_10km':count_same_day\n",
    "                     }) \n",
    "    \n",
    "    indices,distances = kd_time_tree.query_radius([[row['longitude'],row['latitude'],row['timestamp']]],\n",
    "                            0.25,return_distance=True,sort_results=True)\n",
    "    indices = indices[0][1:]\n",
    "    \n",
    "    count_same_day = 0\n",
    "    lat_same_day_nearest = 0\n",
    "    lon_same_day_nearest = 0\n",
    "    distance_same_day_nearest = -1\n",
    "    bearing_same_day_nearest = -200   \n",
    "    if len(indices) > 0:\n",
    "        lat_same_day_nearest = df_train.iloc[indices[0]]['latitude']\n",
    "        lon_same_day_nearest = df_train.iloc[indices[0]]['longitude']\n",
    "        distance_same_day_nearest = calc_distance(row['longitude'],row['latitude'],\n",
    "                                                   lon_same_day_nearest,lat_same_day_nearest)\n",
    "        bearing_same_day_nearest = calc_bearing(row['longitude'],row['latitude'],\n",
    "                                                  lon_same_day_nearest,lat_same_day_nearest)\n",
    "        count_same_day = len(indices)\n",
    "    merge_dict.update({'same_day_nearest_latitude_25km':lat_same_day_nearest,\n",
    "                      'same_day_nearest_longitude_25km':lon_same_day_nearest,\n",
    "                      'same_day_nearest_distance_25km':distance_same_day_nearest,\n",
    "                      'same_day_nearest_bearing_25km':bearing_same_day_nearest,\n",
    "                      'same_day_count_25km':count_same_day\n",
    "                     })\n",
    "    \n",
    "    indices,distances = kd_tree.query_radius([[row['longitude'],row['latitude']]],0.2,\n",
    "                                    return_distance=True,sort_results=True)\n",
    "    indices = indices[0][1:]\n",
    "    if len(indices) > 0:\n",
    "        all_condition = (indices <= max_date_dict[end_year])  \n",
    "        filtered_ind = np.where(all_condition)[0]\n",
    "        merge_dict.update({'20km_count':len(filtered_ind)})\n",
    "        if merge_dict['20km_count'] > 0:\n",
    "            merge_dict['5km_divide_20km'] = merge_dict['5km_count']/merge_dict['20km_count']\n",
    "        else:\n",
    "            merge_dict['5km_divide_20km'] = -1\n",
    "        #except:\n",
    "            #print(len(indices),indices,max_date_dict[end_year],len(filtered_ind),merge_dict['20km_count'],\n",
    "                # merge_dict['5km_count'])\n",
    "            #raise\n",
    "    else:\n",
    "        merge_dict.update({'20km_count':0})\n",
    "        merge_dict['5km_divide_20km'] = -1\n",
    "    \n",
    "    return pd.Series(merge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making raw features...\n",
      "Making raw features done! Time 2.26\n",
      "(174871, 6)\n",
      "Reading winds dataset...\n",
      "uwnd\n",
      "vwnd\n",
      "Reading dataset done! Time 72.68\n",
      "Making wind features...\n",
      "2019-11-13 00:00:00 2012-01-01 00:00:00\n",
      "Making wind features done! Time 1961.27\n",
      "(174871, 308)\n",
      "(174871, 313)\n",
      "Reading temperature-rhum-dataset...\n",
      "rhum\n",
      "air\n",
      "Reading dataset done! Time 65.84\n",
      "Making temperature-humidity features...\n",
      "2019-11-13 00:00:00 2012-01-01 00:00:00\n",
      "(174871, 540)\n",
      "Making temperature-humidity features done! Time 2034.18\n"
     ]
    }
   ],
   "source": [
    "fm = FeaturesMaker()\n",
    "features_df = fm.make_all_features(df_train.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174871, 852)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 16s\n"
     ]
    }
   ],
   "source": [
    "%time raw_features_df = df_train[['latitude','longitude','date','timestamp']].apply(return_nearest_previous_coord,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174871, 875)\n",
      "(174871, 727)\n",
      "(174871,)\n"
     ]
    }
   ],
   "source": [
    "X = pd.merge(features_df,raw_features_df,left_index=True,right_index=True)\n",
    "print(X.shape)\n",
    "X.drop(X.count()[X.count() <= 0].index,axis=1,inplace=True)\n",
    "X.replace({np.inf:np.nan,-np.inf:np.nan},inplace=True)\n",
    "X.fillna(method='ffill',inplace=True)\n",
    "X.fillna(method='bfill',inplace=True)\n",
    "print(X.shape)\n",
    "\n",
    "y = df_train['fire_type']\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "700\n",
      "500\n",
      "300\n",
      "250\n",
      "200\n",
      "100\n",
      "150\n",
      "70\n",
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nimport json\\nimport io\\n\\nwith io.open('transform_dict-2.json','w',encoding='utf-8') as f:\\n    f.write(json.dumps(transform_dict))     \\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут фичи, которые получаются путем агрегации разных фич по координатам сетки ncep.\n",
    "# transform_dict я потом задампил, чтобы сохранить и использовать в скрипте\n",
    "# это последние закоменченные строчки \n",
    "\n",
    "var_dict = {'t':'temperature','h':'humidity','uw':'uwind'}\n",
    "\n",
    "transform_dict = {}\n",
    "\n",
    "X['lat-lon'] = X['lat'].astype(str) + '-' + X['lon'].astype(str)\n",
    "for level in fm.base_levels:\n",
    "    print(level)\n",
    "    for var in ['t','h','uw']:\n",
    "        try:\n",
    "            col = 'p14d_diff-{}_level{}_latlon'.format(var,level)\n",
    "            X[col]= X.groupby(['lat','lon'])['p14d_diff-{}_level{}'.format(var,level)].transform('mean')\n",
    "            transform_dict.update({col:X.drop_duplicates('lat-lon',keep='first')\\\n",
    "                                       .set_index('lat-lon')[col].to_dict()})\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            col = 'diff_first_mean-{}_level{}_latlon'.format(var,level)\n",
    "            X[col] = X.groupby(['lat','lon'])['diff_first_mean-{}_level{}'.format(var,level)].transform('mean')  \n",
    "            transform_dict.update({col:X.drop_duplicates('lat-lon',keep='first')\\\n",
    "                                       .set_index('lat-lon')[col].to_dict()})\n",
    "        except:\n",
    "            pass\n",
    "        new_var = var_dict[var]\n",
    "        try:\n",
    "            col = '{}_level{}_latlon'.format(new_var,level)\n",
    "            X[col] = X.groupby(['lat','lon'])['{}_level{}'.format(new_var,level)].transform('mean')  \n",
    "            transform_dict.update({col:X.drop_duplicates('lat-lon',keep='first')\\\n",
    "                                       .set_index('lat-lon')[col].to_dict()})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "'''\n",
    "import json\n",
    "import io\n",
    "\n",
    "with io.open('transform_dict-2.json','w',encoding='utf-8') as f:\n",
    "    f.write(json.dumps(transform_dict))     \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174871, 800)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174871, 813)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# фичи с городами\n",
    "biggest_cities = pd.read_csv('biggest_cities.csv')\n",
    "biggest_cities.loc[923,'Население'] = 400\n",
    "biggest_cities['Население'] = biggest_cities['Население'].astype(np.int)\n",
    "for _ind in [506,782,863]:\n",
    "    biggest_cities.loc[_ind,'Город'] = biggest_cities.loc[_ind,'Регион']\n",
    "biggest_cities.loc[1066,'Город'] = 'Урус-Мартан'\n",
    "biggest_cities = biggest_cities[['Широта','Долгота','Население','Город']]\n",
    "\n",
    "kd_tree_cities = KDTree(biggest_cities[['Долгота','Широта']])\n",
    "X['city_lon'] = biggest_cities.iloc[kd_tree_cities.query(X[['longitude','latitude']],1,return_distance=False)[:,0]]['Долгота'].values\n",
    "X['city_lat'] = biggest_cities.iloc[kd_tree_cities.query(X[['longitude','latitude']],1,return_distance=False)[:,0]]['Широта'].values\n",
    "X['city_population'] = biggest_cities.iloc[kd_tree_cities.query(X[['longitude','latitude']],1,return_distance=False)[:,0]]['Население'].values\n",
    "X['distance_from_city'] = X[['city_lon','city_lat','latitude','longitude']]\\\n",
    "                           .apply(lambda x: calc_distance(x['city_lon'],x['city_lat'],\n",
    "                                                x['longitude'],x['latitude']),axis=1)\n",
    "\n",
    "# ------------------------------------------\n",
    "# фичи с полями, лесами и заповедниками\n",
    "for _file in ['field_coords','forest_coords','nature_forests']:\n",
    "    forest_df = pd.read_csv('{}.csv'.format(_file))\n",
    "    kd_tree_nature = KDTree(forest_df[['longitude','latitude']])\n",
    "    X['{}_lon'.format(_file)] = forest_df.iloc[kd_tree_nature.query(X[['longitude','latitude']],1,return_distance=False)[:,0]]['longitude'].values\n",
    "    X['{}_lat'.format(_file)] = forest_df.iloc[kd_tree_nature.query(X[['longitude','latitude']],1,return_distance=False)[:,0]]['latitude'].values\n",
    "    X['distance_from_{}'.format(_file)] = X[['{}_lon'.format(_file),'{}_lat'.format(_file),'latitude','longitude']]\\\n",
    "                           .apply(lambda x: calc_distance(x['{}_lon'.format(_file)],x['{}_lat'.format(_file)],\n",
    "                                               x['longitude'],x['latitude']),axis=1)\n",
    "    \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ниже два метода для кросс-валидации\n",
    "\n",
    "У каждого из них одинаковый подход к кросс-валидации:\n",
    "* задается число фолдов n\n",
    "* для k от 0 до n-1:\n",
    "* из выборки удаляются последние k месяцев\n",
    "* тест - последние 4 месяца из оставшейся выборки, трейн - все остальное\n",
    "\n",
    "Кросс-валидация по времени нужна, чтобы не оверфитнуться на какой-то конкретный период - решение сабмитил только в том случае, когда улучшение было только на всех фолдах (кроме преодоления рубежа 0.9297, до 0.9305 частично дошел пробингом, там на некоторых фолдах было улучшение, на некоторых нет)\n",
    "\n",
    "Первый метод - обычная кросс-валидация, нужна для проверки качества\n",
    "\n",
    "Второй метод - кросс-валидация с отбором фич по Permutation Importance. Почему-то eli5 работал дольше, чем написанный вручную и кстати написал я его не совсем правильно :) Моя реализация:\n",
    "* вначале считаем качество (roc-auc) на тестовой выборке\n",
    "* для каждой фичи проделываем следующую процедуру несколько раз (в моей реализации 5):\n",
    "<ol>\n",
    "    <li> перемешиваем в тесте значения этой фичи, повторно вычисляем roc-auc с перемешанной фичей; </li>\n",
    "    <li> считаем разницу между исходным auc и auc с перемешанной фичей. </li>\n",
    "    <li> чем больше эта разница, тем важнее фича (по крайней мере для метрики auc). </li>\n",
    "</ol>\n",
    "\n",
    "Почему у меня неправильная реализация - в Permutation Importance нужно вычислить распределение конкретного признака и случайным образом из этого распределения заполнить значения в тесте. Чтобы узнать распределение нужно использовать весь датасет (трейн+тест), а я тупо перемешивал значения из теста. Таким образом, например, по фиче \"year\" разница между ихсодным auc и перемешанным всегда 0, т.к. при моей реализации кросс-валидации в тесте год всегда единственное значение и перемешивание ничего не меняло.\n",
    "\n",
    "За счет отбора фич по permutation importance я смог прыгнуть с 0.9253 до 0.9289\n",
    "\n",
    "***Permutation Importance работает довольно долго (у меня 2.5 дня), поэтому сюда это я включать не буду. Можно запустить самому***\n",
    "\n",
    "<code>cross_val_permutation_importance(X,y-1,df_train,folds=4,cat_features=[],features_list=list(X.columns))</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "lb.fit(df_train['fire_type']-1)\n",
    "\n",
    "def lgb_auc(preds,train):\n",
    "    '''\n",
    "        кастомная метрика для micro roc-auc для lightgbm\n",
    "        preds - np.array, предсказания от lightgbm\n",
    "        train - lgb.Dataset, обучающая выборка\n",
    "    '''\n",
    "    y = train.get_label()\n",
    "    a = preds.reshape(11,-1).T\n",
    "    return 'roc_auc',roc_auc_score(lb.transform(y),a,average='micro'),True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X,y,df_dates,folds=3,features_list=None,cat_features=None,\n",
    "                           params_lgb=None,file_name='feature-importances-cv.csv'):\n",
    "    '''\n",
    "        стандартная кросс-валидация\n",
    "        \n",
    "        X - признаки\n",
    "        y - лейблы\n",
    "        df_dates - исходный датасет (нужен чтобы на каждом фолде правильно \n",
    "                                     заполнить временные фичи за прошлый год \n",
    "                                    + для разбиения фолдов по дате)\n",
    "        features_list - список используемых фичей\n",
    "        cat_features - список фичей, который будут категориальными\n",
    "        params_lgb - параметры для lightgbm\n",
    "        file_name - имя файла, куда будет сохранено feature importance (от lightgbm, importance по количеству разбиений)\n",
    "    '''\n",
    "    if cat_features is None:\n",
    "        cat_features=['month','year']\n",
    "    #df_dates['date'] = pd.to_datetime(df_dates['date'])\n",
    "    df_dates.sort_values('date',inplace=True)\n",
    "    feature_importances = pd.DataFrame(columns=['fold','date-diapason','feature','importance'])\n",
    "    dif_months = 4\n",
    "    for fold in range(folds):\n",
    "        #if fold <= 3 or fold in [5,6]:\n",
    "         #   continue\n",
    "        #if fold in [1,2]:\n",
    "        max_date = datetime.date(2019,5,1) - relativedelta(months=dif_months*(fold))\n",
    "        #else:\n",
    "        #max_date = datetime.date(2019,5,1)\n",
    "        last_date = max_date - relativedelta(months=dif_months)\n",
    "        print(datetime.date(last_date.year,1,1))\n",
    "        prev_year_ind = df_dates[df_dates['date'] < datetime.date(last_date.year,1,1)].index\n",
    "        now_ind = df_dates[df_dates['date'] >= datetime.date(last_date.year,1,1)].index\n",
    "        print(X.shape)\n",
    "        X_feat = time_series_features(df_dates,X.loc[prev_year_ind],X.loc[now_ind])\n",
    "        print(X_feat.shape)\n",
    "        print(list(X_feat.columns.difference(X.columns)))\n",
    "        print(max_date,last_date)\n",
    "\n",
    "        train_ind = df_dates[df_dates['date'] < last_date].index\n",
    "        test_ind = df_dates[(df_dates['date'] >= last_date) & (df_dates['date'] < max_date)].index\n",
    "        print('Train length {}.  Test length {}'.format(len(train_ind),len(test_ind)))\n",
    "        X_train, y_train = X_feat.loc[train_ind],y.loc[train_ind]\n",
    "        X_test, y_test = X_feat.loc[test_ind],y.loc[test_ind]\n",
    "        \n",
    "        if params_lgb is None:\n",
    "            params_lgb = {\n",
    "                'bagging_freq': 5,\n",
    "                'bagging_fraction': 0.42,\n",
    "                'feature_fraction':0.4,\n",
    "                'bagging_seed':400,\n",
    "                'feature_fraction_seed':400,\n",
    "                'boost': 'gbdt',\n",
    "                'learning_rate': 0.008,\n",
    "                'min_data_in_leaf':4,\n",
    "                'num_leaves': 13,\n",
    "                'num_threads': 6,\n",
    "                'tree_learner': 'voting',\n",
    "                'objective': 'multiclass',\n",
    "                'verbosity': 1,\n",
    "                'lambda_l2':2,\n",
    "                'num_classes': 11,\n",
    "                'seed':4567\n",
    "            }\n",
    "        print(params_lgb)\n",
    "\n",
    "        print(len(features_list))\n",
    "        train_data = lgb.Dataset(X_train[features_list], label=y_train.values)\n",
    "        test_data = lgb.Dataset(X_test[features_list], label=y_test.values)\n",
    "        \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # я здесь убрал метрики для train, т.к. с ними намного дольше\n",
    "        # а по качеству ни разу не было останова из-за трейна\n",
    "        lgb_model = lgb.train(params_lgb,train_data,num_boost_round=20000,\n",
    "                            valid_sets = [test_data],verbose_eval=100,early_stopping_rounds = 400,\n",
    "                                  feature_name = features_list,feval=lgb_auc,\n",
    "                                 categorical_feature = cat_features) \n",
    "        lgb_model.save_model('best-features-{}-fold-{}.dmp'.format(len(features_list),fold))\n",
    "        part_feature_importances = pd.DataFrame(columns=['fold','date-diapason','feature','importance'])\n",
    "        n = 0\n",
    "        for feature_name,importance in zip(lgb_model.feature_name(),lgb_model.feature_importance()):\n",
    "            part_feature_importances.loc[n] = [fold + 1,'{}-{}'.format(last_date.strftime('%Y-%m-%d'),\n",
    "                                        max_date.strftime('%Y-%m-%d')),feature_name,int(importance)]\n",
    "            n += 1\n",
    "        feature_importances = pd.concat([feature_importances,part_feature_importances],ignore_index=True,axis=0)\n",
    "        feature_importances.to_csv(file_name)\n",
    "    return feature_importances\n",
    "\n",
    "\n",
    "def cross_val_permutation_importance(X,y,df_dates,folds=3,features_list=None,\n",
    "                   cat_features=None,file_name='permutation-feature-importances.csv',\n",
    "                                    permutation_iters=5):\n",
    "    '''\n",
    "        кросс-валидация с вычислением permutation importance\n",
    "        \n",
    "        X - признаки\n",
    "        y - лейблы\n",
    "        df_dates - исходный датасет (нужен чтобы на каждом фолде правильно \n",
    "                                     заполнить временные фичи за прошлый год \n",
    "                                    + для разбиения фолдов по дате)\n",
    "        features_list - список используемых фичей\n",
    "        cat_features - список фичей, который будут категориальными\n",
    "        params_lgb - параметры для lightgbm\n",
    "        file_name - имя файла, куда будет сохранено feature importance \n",
    "                               (от lightgbm, importance по количеству разбиений +\n",
    "                               permutation importance)\n",
    "        permutation_iters - количество итераций при перемешивании признака в permutation importance\n",
    "    '''\n",
    "    if cat_features is None:\n",
    "        cat_features=['month','year']\n",
    "    #df_dates['date'] = pd.to_datetime(df_dates['date'])\n",
    "    df_dates.sort_values('date',inplace=True)\n",
    "    feature_importances_df = pd.DataFrame(columns=['fold','date-diapason','feature','importance'])\n",
    "    dif_months = 4\n",
    "    \n",
    "    for fold in range(folds):\n",
    "        if fold <= 1:\n",
    "            continue\n",
    "        #if fold in [1,2]:\n",
    "        max_date = datetime.date(2019,5,1) - relativedelta(months=dif_months*(fold))\n",
    "        #else:\n",
    "        #max_date = datetime.date(2019,5,1)\n",
    "        last_date = max_date - relativedelta(months=dif_months)\n",
    "        print(datetime.date(last_date.year,1,1))\n",
    "        prev_year_ind = df_dates[df_dates['date'] < datetime.date(last_date.year,1,1)].index\n",
    "        now_ind = df_dates[df_dates['date'] >= datetime.date(last_date.year,1,1)].index\n",
    "        print(X.shape)\n",
    "        X_feat = time_series_features(df_dates,X.loc[prev_year_ind],X.loc[now_ind])\n",
    "        print(X_feat.shape)\n",
    "        print(list(X_feat.columns.difference(X.columns)))\n",
    "        print(max_date,last_date)\n",
    "\n",
    "        train_ind = df_dates[df_dates['date'] < last_date].index\n",
    "        test_ind = df_dates[(df_dates['date'] >= last_date) & (df_dates['date'] < max_date)].index\n",
    "        print('Train length {}.  Test length {}'.format(len(train_ind),len(test_ind)))\n",
    "        X_train, y_train = X_feat.loc[train_ind],y.loc[train_ind]\n",
    "        X_test, y_test = X_feat.loc[test_ind],y.loc[test_ind]\n",
    "        \n",
    "        params_lgb = {\n",
    "            'bagging_freq': 5,\n",
    "            'bagging_fraction': 0.41,\n",
    "            'feature_fraction':0.4,\n",
    "            'bagging_seed':400,\n",
    "            'feature_fraction_seed':400,\n",
    "            'boost': 'gbdt',\n",
    "            'learning_rate': 0.008,\n",
    "            'min_data_in_leaf':4,\n",
    "            'num_leaves': 13,\n",
    "            'num_threads': 6,\n",
    "            'tree_learner': 'voting',\n",
    "            'objective': 'multiclass',\n",
    "            'verbosity': 1,\n",
    "            'lambda_l2':2,\n",
    "            'num_classes': 11,\n",
    "            'seed':4567\n",
    "        }\n",
    "\n",
    "        feature_names = list(X_feat.columns)\n",
    "        train_data = lgb.Dataset(X_train[features_list], label=y_train.values)\n",
    "        print('features count: {}'.format(len(train_data.feature_name)))\n",
    "        test_data = lgb.Dataset(X_test[features_list], label=y_test.values)\n",
    "        \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # я здесь убрал метрики для train, т.к. с ними намного дольше\n",
    "        # а по качеству ни разу не было останова из-за трейна\n",
    "        lgb_model = lgb.train(params_lgb,train_data,num_boost_round=10000,\n",
    "                            valid_sets = [test_data],verbose_eval=100,early_stopping_rounds = 400,\n",
    "                                  feature_name =features_list,feval=lgb_auc,\n",
    "                                 categorical_feature = cat_features)\n",
    "        \n",
    "        part_feature_importances = pd.DataFrame(columns=['fold','date-diapason','iter','feature',\n",
    "                                                         'permutation-importance','importance'])\n",
    "        n = 0\n",
    "        start = time.time()\n",
    "        good_preds = lgb_model.predict(X_test[feature_names])\n",
    "        good_auc = roc_auc_score(lb.transform(y_test),good_preds,average='micro')\n",
    "        for feature_name,imp in zip(lgb_model.feature_name(),lgb_model.feature_importance()):\n",
    "            a = X_test[feature_name].copy()\n",
    "            for _iter in range(4):\n",
    "                X_test[feature_name] = X_test[feature_name].sample(frac=1).values\n",
    "                preds = lgb_model.predict(X_test[feature_names])\n",
    "                permutation_auc = roc_auc_score(lb.transform(y_test),preds,average='micro')\n",
    "                part_feature_importances.loc[n] = [fold + 1,'{}-{}'.format(last_date.strftime('%Y-%m-%d'),\n",
    "                                            max_date.strftime('%Y-%m-%d')),_iter,feature_name,\n",
    "                                                   good_auc-permutation_auc,imp]\n",
    "                n += 1\n",
    "                #part_feature_importances.to_csv(file_name)\n",
    "            X_test[feature_name] = a.copy()\n",
    "        end = time.time()\n",
    "        print(end-start)\n",
    "        feature_importances_df = pd.concat([feature_importances_df,part_feature_importances],ignore_index=True,axis=0)\n",
    "        feature_importances_df.to_csv(file_name)\n",
    "    return feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01\n",
      "(174871, 813)\n",
      "(174871, 822)\n",
      "['count_035', 'count_035_divide_count_15', 'count_15', 'dist_035_latitude_mean', 'dist_035_longitude_mean', 'dist_035_square', 'dist_15_square', 'distance_from_035', 'square_ratio']\n",
      "2019-05-01 2019-01-01\n",
      "Train length 158860.  Test length 16011\n",
      "{'bagging_freq': 5, 'bagging_fraction': 0.42, 'feature_fraction': 0.4, 'bagging_seed': 400, 'feature_fraction_seed': 400, 'boost': 'gbdt', 'learning_rate': 0.008, 'min_data_in_leaf': 4, 'num_leaves': 13, 'num_threads': 6, 'tree_learner': 'voting', 'objective': 'multiclass', 'verbosity': 1, 'lambda_l2': 2, 'num_classes': 11, 'seed': 4567}\n",
      "214\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[100]\tvalid_0's multi_logloss: 1.4903\tvalid_0's roc_auc: 0.906967\n",
      "[200]\tvalid_0's multi_logloss: 1.36509\tvalid_0's roc_auc: 0.917348\n",
      "[300]\tvalid_0's multi_logloss: 1.30469\tvalid_0's roc_auc: 0.920515\n",
      "[400]\tvalid_0's multi_logloss: 1.26994\tvalid_0's roc_auc: 0.922576\n",
      "[500]\tvalid_0's multi_logloss: 1.24978\tvalid_0's roc_auc: 0.923809\n",
      "[600]\tvalid_0's multi_logloss: 1.23685\tvalid_0's roc_auc: 0.924533\n",
      "[700]\tvalid_0's multi_logloss: 1.2284\tvalid_0's roc_auc: 0.92507\n",
      "[800]\tvalid_0's multi_logloss: 1.22162\tvalid_0's roc_auc: 0.925598\n",
      "[900]\tvalid_0's multi_logloss: 1.21585\tvalid_0's roc_auc: 0.926073\n",
      "[1000]\tvalid_0's multi_logloss: 1.2114\tvalid_0's roc_auc: 0.926455\n",
      "[1100]\tvalid_0's multi_logloss: 1.20847\tvalid_0's roc_auc: 0.926699\n",
      "[1200]\tvalid_0's multi_logloss: 1.20572\tvalid_0's roc_auc: 0.926969\n",
      "[1300]\tvalid_0's multi_logloss: 1.20394\tvalid_0's roc_auc: 0.92711\n",
      "[1400]\tvalid_0's multi_logloss: 1.20166\tvalid_0's roc_auc: 0.927342\n",
      "[1500]\tvalid_0's multi_logloss: 1.19961\tvalid_0's roc_auc: 0.927543\n",
      "[1600]\tvalid_0's multi_logloss: 1.19789\tvalid_0's roc_auc: 0.927721\n",
      "[1700]\tvalid_0's multi_logloss: 1.19658\tvalid_0's roc_auc: 0.927853\n",
      "[1800]\tvalid_0's multi_logloss: 1.19533\tvalid_0's roc_auc: 0.927976\n",
      "[1900]\tvalid_0's multi_logloss: 1.19411\tvalid_0's roc_auc: 0.928119\n",
      "[2000]\tvalid_0's multi_logloss: 1.1932\tvalid_0's roc_auc: 0.928219\n",
      "[2100]\tvalid_0's multi_logloss: 1.19258\tvalid_0's roc_auc: 0.928272\n",
      "[2200]\tvalid_0's multi_logloss: 1.19188\tvalid_0's roc_auc: 0.928365\n",
      "[2300]\tvalid_0's multi_logloss: 1.19104\tvalid_0's roc_auc: 0.928455\n",
      "[2400]\tvalid_0's multi_logloss: 1.18983\tvalid_0's roc_auc: 0.928595\n",
      "[2500]\tvalid_0's multi_logloss: 1.18949\tvalid_0's roc_auc: 0.92862\n",
      "[2600]\tvalid_0's multi_logloss: 1.18878\tvalid_0's roc_auc: 0.928701\n",
      "[2700]\tvalid_0's multi_logloss: 1.18831\tvalid_0's roc_auc: 0.928756\n",
      "[2800]\tvalid_0's multi_logloss: 1.18797\tvalid_0's roc_auc: 0.928785\n",
      "[2900]\tvalid_0's multi_logloss: 1.18768\tvalid_0's roc_auc: 0.928817\n",
      "[3000]\tvalid_0's multi_logloss: 1.18702\tvalid_0's roc_auc: 0.928895\n",
      "[3100]\tvalid_0's multi_logloss: 1.18643\tvalid_0's roc_auc: 0.928979\n",
      "[3200]\tvalid_0's multi_logloss: 1.18589\tvalid_0's roc_auc: 0.929053\n",
      "[3300]\tvalid_0's multi_logloss: 1.18571\tvalid_0's roc_auc: 0.929069\n",
      "[3400]\tvalid_0's multi_logloss: 1.18534\tvalid_0's roc_auc: 0.929116\n",
      "[3500]\tvalid_0's multi_logloss: 1.18466\tvalid_0's roc_auc: 0.929205\n",
      "[3600]\tvalid_0's multi_logloss: 1.18449\tvalid_0's roc_auc: 0.929225\n",
      "[3700]\tvalid_0's multi_logloss: 1.18396\tvalid_0's roc_auc: 0.929297\n",
      "[3800]\tvalid_0's multi_logloss: 1.18405\tvalid_0's roc_auc: 0.929294\n",
      "[3900]\tvalid_0's multi_logloss: 1.18376\tvalid_0's roc_auc: 0.929338\n",
      "[4000]\tvalid_0's multi_logloss: 1.18368\tvalid_0's roc_auc: 0.929352\n",
      "[4100]\tvalid_0's multi_logloss: 1.18353\tvalid_0's roc_auc: 0.929367\n",
      "[4200]\tvalid_0's multi_logloss: 1.18276\tvalid_0's roc_auc: 0.92946\n",
      "[4300]\tvalid_0's multi_logloss: 1.18263\tvalid_0's roc_auc: 0.929484\n",
      "[4400]\tvalid_0's multi_logloss: 1.18249\tvalid_0's roc_auc: 0.929505\n",
      "[4500]\tvalid_0's multi_logloss: 1.18227\tvalid_0's roc_auc: 0.929538\n",
      "[4600]\tvalid_0's multi_logloss: 1.18233\tvalid_0's roc_auc: 0.92954\n",
      "[4700]\tvalid_0's multi_logloss: 1.18226\tvalid_0's roc_auc: 0.929561\n",
      "[4800]\tvalid_0's multi_logloss: 1.1821\tvalid_0's roc_auc: 0.929594\n",
      "[4900]\tvalid_0's multi_logloss: 1.18186\tvalid_0's roc_auc: 0.929628\n",
      "[5000]\tvalid_0's multi_logloss: 1.18184\tvalid_0's roc_auc: 0.92964\n",
      "[5100]\tvalid_0's multi_logloss: 1.18204\tvalid_0's roc_auc: 0.929622\n",
      "[5200]\tvalid_0's multi_logloss: 1.18186\tvalid_0's roc_auc: 0.929653\n",
      "[5300]\tvalid_0's multi_logloss: 1.18205\tvalid_0's roc_auc: 0.929639\n",
      "Early stopping, best iteration is:\n",
      "[4953]\tvalid_0's multi_logloss: 1.18162\tvalid_0's roc_auc: 0.929665\n",
      "2018-01-01\n",
      "(174871, 813)\n",
      "(174871, 822)\n",
      "['count_035', 'count_035_divide_count_15', 'count_15', 'dist_035_latitude_mean', 'dist_035_longitude_mean', 'dist_035_square', 'dist_15_square', 'distance_from_035', 'square_ratio']\n",
      "2019-01-01 2018-09-01\n",
      "Train length 152409.  Test length 6451\n",
      "{'bagging_freq': 5, 'bagging_fraction': 0.42, 'feature_fraction': 0.4, 'bagging_seed': 400, 'feature_fraction_seed': 400, 'boost': 'gbdt', 'learning_rate': 0.008, 'min_data_in_leaf': 4, 'num_leaves': 13, 'num_threads': 6, 'tree_learner': 'voting', 'objective': 'multiclass', 'verbosity': 1, 'lambda_l2': 2, 'num_classes': 11, 'seed': 4567}\n",
      "214\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[100]\tvalid_0's multi_logloss: 1.7582\tvalid_0's roc_auc: 0.833212\n",
      "[200]\tvalid_0's multi_logloss: 1.60192\tvalid_0's roc_auc: 0.864863\n",
      "[300]\tvalid_0's multi_logloss: 1.52093\tvalid_0's roc_auc: 0.878731\n",
      "[400]\tvalid_0's multi_logloss: 1.47092\tvalid_0's roc_auc: 0.886961\n",
      "[500]\tvalid_0's multi_logloss: 1.4386\tvalid_0's roc_auc: 0.892628\n",
      "[600]\tvalid_0's multi_logloss: 1.41236\tvalid_0's roc_auc: 0.897351\n",
      "[700]\tvalid_0's multi_logloss: 1.39329\tvalid_0's roc_auc: 0.900621\n",
      "[800]\tvalid_0's multi_logloss: 1.37973\tvalid_0's roc_auc: 0.902844\n",
      "[900]\tvalid_0's multi_logloss: 1.36915\tvalid_0's roc_auc: 0.904518\n",
      "[1000]\tvalid_0's multi_logloss: 1.36139\tvalid_0's roc_auc: 0.905791\n",
      "[1100]\tvalid_0's multi_logloss: 1.35387\tvalid_0's roc_auc: 0.907036\n",
      "[1200]\tvalid_0's multi_logloss: 1.34837\tvalid_0's roc_auc: 0.907911\n",
      "[1300]\tvalid_0's multi_logloss: 1.34454\tvalid_0's roc_auc: 0.908581\n",
      "[1400]\tvalid_0's multi_logloss: 1.34092\tvalid_0's roc_auc: 0.909138\n",
      "[1500]\tvalid_0's multi_logloss: 1.33781\tvalid_0's roc_auc: 0.909625\n",
      "[1600]\tvalid_0's multi_logloss: 1.33502\tvalid_0's roc_auc: 0.910053\n",
      "[1700]\tvalid_0's multi_logloss: 1.33258\tvalid_0's roc_auc: 0.910456\n",
      "[1800]\tvalid_0's multi_logloss: 1.33049\tvalid_0's roc_auc: 0.910755\n",
      "[1900]\tvalid_0's multi_logloss: 1.32933\tvalid_0's roc_auc: 0.910967\n",
      "[2000]\tvalid_0's multi_logloss: 1.32812\tvalid_0's roc_auc: 0.91117\n",
      "[2100]\tvalid_0's multi_logloss: 1.32711\tvalid_0's roc_auc: 0.911315\n",
      "[2200]\tvalid_0's multi_logloss: 1.32625\tvalid_0's roc_auc: 0.911451\n",
      "[2300]\tvalid_0's multi_logloss: 1.325\tvalid_0's roc_auc: 0.91165\n",
      "[2400]\tvalid_0's multi_logloss: 1.32411\tvalid_0's roc_auc: 0.911784\n",
      "[2500]\tvalid_0's multi_logloss: 1.32317\tvalid_0's roc_auc: 0.911941\n",
      "[2600]\tvalid_0's multi_logloss: 1.32266\tvalid_0's roc_auc: 0.912026\n",
      "[2700]\tvalid_0's multi_logloss: 1.32215\tvalid_0's roc_auc: 0.912108\n",
      "[2800]\tvalid_0's multi_logloss: 1.32186\tvalid_0's roc_auc: 0.912161\n",
      "[2900]\tvalid_0's multi_logloss: 1.32137\tvalid_0's roc_auc: 0.912248\n",
      "[3000]\tvalid_0's multi_logloss: 1.3198\tvalid_0's roc_auc: 0.912456\n",
      "[3100]\tvalid_0's multi_logloss: 1.31878\tvalid_0's roc_auc: 0.912611\n",
      "[3200]\tvalid_0's multi_logloss: 1.31875\tvalid_0's roc_auc: 0.912631\n",
      "[3300]\tvalid_0's multi_logloss: 1.31819\tvalid_0's roc_auc: 0.912722\n",
      "[3400]\tvalid_0's multi_logloss: 1.31814\tvalid_0's roc_auc: 0.912747\n",
      "[3500]\tvalid_0's multi_logloss: 1.31709\tvalid_0's roc_auc: 0.912903\n",
      "[3600]\tvalid_0's multi_logloss: 1.31725\tvalid_0's roc_auc: 0.912908\n",
      "[3700]\tvalid_0's multi_logloss: 1.3166\tvalid_0's roc_auc: 0.913025\n",
      "[3800]\tvalid_0's multi_logloss: 1.3169\tvalid_0's roc_auc: 0.913001\n",
      "[3900]\tvalid_0's multi_logloss: 1.31647\tvalid_0's roc_auc: 0.913086\n",
      "[4000]\tvalid_0's multi_logloss: 1.31609\tvalid_0's roc_auc: 0.913166\n",
      "[4100]\tvalid_0's multi_logloss: 1.31539\tvalid_0's roc_auc: 0.913285\n",
      "[4200]\tvalid_0's multi_logloss: 1.31572\tvalid_0's roc_auc: 0.913276\n",
      "[4300]\tvalid_0's multi_logloss: 1.31583\tvalid_0's roc_auc: 0.91329\n",
      "[4400]\tvalid_0's multi_logloss: 1.31593\tvalid_0's roc_auc: 0.91329\n",
      "[4500]\tvalid_0's multi_logloss: 1.31591\tvalid_0's roc_auc: 0.913301\n",
      "Early stopping, best iteration is:\n",
      "[4140]\tvalid_0's multi_logloss: 1.31535\tvalid_0's roc_auc: 0.913306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01\n",
      "(174871, 813)\n",
      "(174871, 822)\n",
      "['count_035', 'count_035_divide_count_15', 'count_15', 'dist_035_latitude_mean', 'dist_035_longitude_mean', 'dist_035_square', 'dist_15_square', 'distance_from_035', 'square_ratio']\n",
      "2018-09-01 2018-05-01\n",
      "Train length 145944.  Test length 6465\n",
      "{'bagging_freq': 5, 'bagging_fraction': 0.42, 'feature_fraction': 0.4, 'bagging_seed': 400, 'feature_fraction_seed': 400, 'boost': 'gbdt', 'learning_rate': 0.008, 'min_data_in_leaf': 4, 'num_leaves': 13, 'num_threads': 6, 'tree_learner': 'voting', 'objective': 'multiclass', 'verbosity': 1, 'lambda_l2': 2, 'num_classes': 11, 'seed': 4567}\n",
      "214\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[100]\tvalid_0's multi_logloss: 1.5392\tvalid_0's roc_auc: 0.894587\n",
      "[200]\tvalid_0's multi_logloss: 1.34917\tvalid_0's roc_auc: 0.920765\n",
      "[300]\tvalid_0's multi_logloss: 1.24701\tvalid_0's roc_auc: 0.929391\n",
      "[400]\tvalid_0's multi_logloss: 1.18258\tvalid_0's roc_auc: 0.934867\n",
      "[500]\tvalid_0's multi_logloss: 1.14229\tvalid_0's roc_auc: 0.938358\n",
      "[600]\tvalid_0's multi_logloss: 1.11124\tvalid_0's roc_auc: 0.940922\n",
      "[700]\tvalid_0's multi_logloss: 1.08991\tvalid_0's roc_auc: 0.942774\n",
      "[800]\tvalid_0's multi_logloss: 1.07429\tvalid_0's roc_auc: 0.944138\n",
      "[900]\tvalid_0's multi_logloss: 1.06105\tvalid_0's roc_auc: 0.945228\n",
      "[1000]\tvalid_0's multi_logloss: 1.05035\tvalid_0's roc_auc: 0.946119\n",
      "[1100]\tvalid_0's multi_logloss: 1.04102\tvalid_0's roc_auc: 0.946915\n",
      "[1200]\tvalid_0's multi_logloss: 1.03492\tvalid_0's roc_auc: 0.947423\n",
      "[1300]\tvalid_0's multi_logloss: 1.02821\tvalid_0's roc_auc: 0.948003\n",
      "[1400]\tvalid_0's multi_logloss: 1.02356\tvalid_0's roc_auc: 0.948369\n",
      "[1500]\tvalid_0's multi_logloss: 1.02036\tvalid_0's roc_auc: 0.948632\n",
      "[1600]\tvalid_0's multi_logloss: 1.01642\tvalid_0's roc_auc: 0.94898\n",
      "[1700]\tvalid_0's multi_logloss: 1.01331\tvalid_0's roc_auc: 0.949232\n",
      "[1800]\tvalid_0's multi_logloss: 1.0102\tvalid_0's roc_auc: 0.949507\n",
      "[1900]\tvalid_0's multi_logloss: 1.00832\tvalid_0's roc_auc: 0.949648\n",
      "[2000]\tvalid_0's multi_logloss: 1.0065\tvalid_0's roc_auc: 0.949783\n",
      "[2100]\tvalid_0's multi_logloss: 1.00445\tvalid_0's roc_auc: 0.949955\n",
      "[2200]\tvalid_0's multi_logloss: 1.00255\tvalid_0's roc_auc: 0.950105\n",
      "[2300]\tvalid_0's multi_logloss: 1.00098\tvalid_0's roc_auc: 0.950234\n",
      "[2400]\tvalid_0's multi_logloss: 0.999385\tvalid_0's roc_auc: 0.950364\n",
      "[2500]\tvalid_0's multi_logloss: 0.998233\tvalid_0's roc_auc: 0.950457\n",
      "[2600]\tvalid_0's multi_logloss: 0.997432\tvalid_0's roc_auc: 0.950498\n",
      "[2700]\tvalid_0's multi_logloss: 0.995694\tvalid_0's roc_auc: 0.950633\n",
      "[2800]\tvalid_0's multi_logloss: 0.994786\tvalid_0's roc_auc: 0.950697\n",
      "[2900]\tvalid_0's multi_logloss: 0.994393\tvalid_0's roc_auc: 0.950714\n",
      "[3000]\tvalid_0's multi_logloss: 0.993196\tvalid_0's roc_auc: 0.950806\n",
      "[3100]\tvalid_0's multi_logloss: 0.992155\tvalid_0's roc_auc: 0.950887\n",
      "[3200]\tvalid_0's multi_logloss: 0.990984\tvalid_0's roc_auc: 0.950979\n",
      "[3300]\tvalid_0's multi_logloss: 0.990137\tvalid_0's roc_auc: 0.951036\n",
      "[3400]\tvalid_0's multi_logloss: 0.988923\tvalid_0's roc_auc: 0.951138\n",
      "[3500]\tvalid_0's multi_logloss: 0.987918\tvalid_0's roc_auc: 0.951217\n",
      "[3600]\tvalid_0's multi_logloss: 0.986572\tvalid_0's roc_auc: 0.951328\n",
      "[3700]\tvalid_0's multi_logloss: 0.985577\tvalid_0's roc_auc: 0.951407\n",
      "[3800]\tvalid_0's multi_logloss: 0.984702\tvalid_0's roc_auc: 0.95148\n",
      "[3900]\tvalid_0's multi_logloss: 0.984034\tvalid_0's roc_auc: 0.951514\n",
      "[4000]\tvalid_0's multi_logloss: 0.983263\tvalid_0's roc_auc: 0.951583\n",
      "[4100]\tvalid_0's multi_logloss: 0.983057\tvalid_0's roc_auc: 0.951585\n",
      "[4200]\tvalid_0's multi_logloss: 0.982346\tvalid_0's roc_auc: 0.951636\n",
      "[4300]\tvalid_0's multi_logloss: 0.981674\tvalid_0's roc_auc: 0.951685\n",
      "[4400]\tvalid_0's multi_logloss: 0.981396\tvalid_0's roc_auc: 0.951705\n",
      "[4500]\tvalid_0's multi_logloss: 0.98071\tvalid_0's roc_auc: 0.951755\n",
      "[4600]\tvalid_0's multi_logloss: 0.979962\tvalid_0's roc_auc: 0.95181\n",
      "[4700]\tvalid_0's multi_logloss: 0.979577\tvalid_0's roc_auc: 0.951836\n",
      "[4800]\tvalid_0's multi_logloss: 0.97916\tvalid_0's roc_auc: 0.951863\n",
      "[4900]\tvalid_0's multi_logloss: 0.978835\tvalid_0's roc_auc: 0.95188\n",
      "[5000]\tvalid_0's multi_logloss: 0.978733\tvalid_0's roc_auc: 0.951881\n",
      "[5100]\tvalid_0's multi_logloss: 0.978305\tvalid_0's roc_auc: 0.951917\n",
      "[5200]\tvalid_0's multi_logloss: 0.977971\tvalid_0's roc_auc: 0.95194\n",
      "[5300]\tvalid_0's multi_logloss: 0.977651\tvalid_0's roc_auc: 0.95196\n",
      "[5400]\tvalid_0's multi_logloss: 0.977595\tvalid_0's roc_auc: 0.951949\n",
      "[5500]\tvalid_0's multi_logloss: 0.977543\tvalid_0's roc_auc: 0.951943\n",
      "[5600]\tvalid_0's multi_logloss: 0.97718\tvalid_0's roc_auc: 0.951967\n",
      "[5700]\tvalid_0's multi_logloss: 0.976701\tvalid_0's roc_auc: 0.95201\n",
      "[5800]\tvalid_0's multi_logloss: 0.976474\tvalid_0's roc_auc: 0.952023\n",
      "[5900]\tvalid_0's multi_logloss: 0.976005\tvalid_0's roc_auc: 0.95206\n",
      "[6000]\tvalid_0's multi_logloss: 0.975561\tvalid_0's roc_auc: 0.952093\n",
      "[6100]\tvalid_0's multi_logloss: 0.97525\tvalid_0's roc_auc: 0.952106\n",
      "[6200]\tvalid_0's multi_logloss: 0.974927\tvalid_0's roc_auc: 0.952129\n",
      "[6300]\tvalid_0's multi_logloss: 0.974589\tvalid_0's roc_auc: 0.952148\n",
      "[6400]\tvalid_0's multi_logloss: 0.974643\tvalid_0's roc_auc: 0.952138\n",
      "[6500]\tvalid_0's multi_logloss: 0.974412\tvalid_0's roc_auc: 0.952155\n",
      "[6600]\tvalid_0's multi_logloss: 0.974315\tvalid_0's roc_auc: 0.952152\n",
      "[6700]\tvalid_0's multi_logloss: 0.974183\tvalid_0's roc_auc: 0.952157\n",
      "[6800]\tvalid_0's multi_logloss: 0.973833\tvalid_0's roc_auc: 0.952188\n",
      "[6900]\tvalid_0's multi_logloss: 0.97369\tvalid_0's roc_auc: 0.952196\n",
      "[7000]\tvalid_0's multi_logloss: 0.973244\tvalid_0's roc_auc: 0.952235\n",
      "[7100]\tvalid_0's multi_logloss: 0.972668\tvalid_0's roc_auc: 0.952277\n",
      "[7200]\tvalid_0's multi_logloss: 0.972715\tvalid_0's roc_auc: 0.952269\n",
      "[7300]\tvalid_0's multi_logloss: 0.972597\tvalid_0's roc_auc: 0.952272\n",
      "[7400]\tvalid_0's multi_logloss: 0.972379\tvalid_0's roc_auc: 0.952288\n",
      "[7500]\tvalid_0's multi_logloss: 0.97219\tvalid_0's roc_auc: 0.952309\n",
      "[7600]\tvalid_0's multi_logloss: 0.971926\tvalid_0's roc_auc: 0.952327\n",
      "[7700]\tvalid_0's multi_logloss: 0.971779\tvalid_0's roc_auc: 0.952341\n",
      "[7800]\tvalid_0's multi_logloss: 0.971815\tvalid_0's roc_auc: 0.952332\n",
      "[7900]\tvalid_0's multi_logloss: 0.971618\tvalid_0's roc_auc: 0.952346\n",
      "[8000]\tvalid_0's multi_logloss: 0.97155\tvalid_0's roc_auc: 0.95235\n",
      "[8100]\tvalid_0's multi_logloss: 0.971382\tvalid_0's roc_auc: 0.952358\n",
      "[8200]\tvalid_0's multi_logloss: 0.971669\tvalid_0's roc_auc: 0.952322\n",
      "[8300]\tvalid_0's multi_logloss: 0.971542\tvalid_0's roc_auc: 0.952328\n",
      "[8400]\tvalid_0's multi_logloss: 0.971437\tvalid_0's roc_auc: 0.952331\n",
      "[8500]\tvalid_0's multi_logloss: 0.971478\tvalid_0's roc_auc: 0.952327\n",
      "Early stopping, best iteration is:\n",
      "[8130]\tvalid_0's multi_logloss: 0.971132\tvalid_0's roc_auc: 0.952381\n",
      "2018-01-01\n",
      "(174871, 813)\n",
      "(174871, 822)\n",
      "['count_035', 'count_035_divide_count_15', 'count_15', 'dist_035_latitude_mean', 'dist_035_longitude_mean', 'dist_035_square', 'dist_15_square', 'distance_from_035', 'square_ratio']\n",
      "2018-05-01 2018-01-01\n",
      "Train length 136040.  Test length 9904\n",
      "{'bagging_freq': 5, 'bagging_fraction': 0.42, 'feature_fraction': 0.4, 'bagging_seed': 400, 'feature_fraction_seed': 400, 'boost': 'gbdt', 'learning_rate': 0.008, 'min_data_in_leaf': 4, 'num_leaves': 13, 'num_threads': 6, 'tree_learner': 'voting', 'objective': 'multiclass', 'verbosity': 1, 'lambda_l2': 2, 'num_classes': 11, 'seed': 4567}\n",
      "214\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[100]\tvalid_0's multi_logloss: 1.56516\tvalid_0's roc_auc: 0.883051\n",
      "[200]\tvalid_0's multi_logloss: 1.44214\tvalid_0's roc_auc: 0.899655\n",
      "[300]\tvalid_0's multi_logloss: 1.37862\tvalid_0's roc_auc: 0.906169\n",
      "[400]\tvalid_0's multi_logloss: 1.34182\tvalid_0's roc_auc: 0.910034\n",
      "[500]\tvalid_0's multi_logloss: 1.32071\tvalid_0's roc_auc: 0.912374\n",
      "[600]\tvalid_0's multi_logloss: 1.30485\tvalid_0's roc_auc: 0.91438\n",
      "[700]\tvalid_0's multi_logloss: 1.29473\tvalid_0's roc_auc: 0.915813\n",
      "[800]\tvalid_0's multi_logloss: 1.28957\tvalid_0's roc_auc: 0.916535\n",
      "[900]\tvalid_0's multi_logloss: 1.28495\tvalid_0's roc_auc: 0.917252\n",
      "[1000]\tvalid_0's multi_logloss: 1.27986\tvalid_0's roc_auc: 0.918062\n",
      "[1100]\tvalid_0's multi_logloss: 1.27737\tvalid_0's roc_auc: 0.918441\n",
      "[1200]\tvalid_0's multi_logloss: 1.27497\tvalid_0's roc_auc: 0.918839\n",
      "[1300]\tvalid_0's multi_logloss: 1.27314\tvalid_0's roc_auc: 0.919175\n",
      "[1400]\tvalid_0's multi_logloss: 1.27144\tvalid_0's roc_auc: 0.919475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500]\tvalid_0's multi_logloss: 1.26926\tvalid_0's roc_auc: 0.919825\n",
      "[1600]\tvalid_0's multi_logloss: 1.27012\tvalid_0's roc_auc: 0.919757\n",
      "[1700]\tvalid_0's multi_logloss: 1.27063\tvalid_0's roc_auc: 0.919757\n",
      "[1800]\tvalid_0's multi_logloss: 1.27024\tvalid_0's roc_auc: 0.919892\n",
      "[1900]\tvalid_0's multi_logloss: 1.26989\tvalid_0's roc_auc: 0.919977\n",
      "Early stopping, best iteration is:\n",
      "[1519]\tvalid_0's multi_logloss: 1.2688\tvalid_0's roc_auc: 0.91988\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>date-diapason</th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01-2019-05-01</td>\n",
       "      <td>p14d-uw_level_100_50_diff</td>\n",
       "      <td>6246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01-2019-05-01</td>\n",
       "      <td>first_uwind_near_2_level_200</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01-2019-05-01</td>\n",
       "      <td>humidity_level300_latlon</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01-2019-05-01</td>\n",
       "      <td>first_uwind_near_3_level_300</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01-2019-05-01</td>\n",
       "      <td>uwind_level100_latlon</td>\n",
       "      <td>1159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-01-2018-05-01</td>\n",
       "      <td>forest_coords_lon</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-01-2018-05-01</td>\n",
       "      <td>distance_from_forest_coords</td>\n",
       "      <td>1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-01-2018-05-01</td>\n",
       "      <td>field_coords_lat</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-01-2018-05-01</td>\n",
       "      <td>field_coords_lon</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-01-2018-05-01</td>\n",
       "      <td>distance_from_field_coords</td>\n",
       "      <td>1770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>856 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold          date-diapason                       feature importance\n",
       "0      1  2019-01-01-2019-05-01     p14d-uw_level_100_50_diff       6246\n",
       "1      1  2019-01-01-2019-05-01  first_uwind_near_2_level_200       1966\n",
       "2      1  2019-01-01-2019-05-01      humidity_level300_latlon       1900\n",
       "3      1  2019-01-01-2019-05-01  first_uwind_near_3_level_300       2013\n",
       "4      1  2019-01-01-2019-05-01         uwind_level100_latlon       1159\n",
       "..   ...                    ...                           ...        ...\n",
       "851    4  2018-01-01-2018-05-01             forest_coords_lon        889\n",
       "852    4  2018-01-01-2018-05-01   distance_from_forest_coords       1414\n",
       "853    4  2018-01-01-2018-05-01              field_coords_lat        756\n",
       "854    4  2018-01-01-2018-05-01              field_coords_lon       1104\n",
       "855    4  2018-01-01-2018-05-01    distance_from_field_coords       1770\n",
       "\n",
       "[856 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# это уже кросс-валидация с отобранными фичами\n",
    "cross_val(X,y-1,df_train,folds=4,cat_features=[],features_list=model_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb={\n",
    "                'bagging_freq': 5,\n",
    "                'bagging_fraction': 0.42,\n",
    "                'feature_fraction':0.4,\n",
    "                'bagging_seed':400,\n",
    "                'feature_fraction_seed':400,\n",
    "                'boost': 'gbdt',\n",
    "                'learning_rate': 0.008,\n",
    "                'min_data_in_leaf':4,\n",
    "                'num_leaves': 13,\n",
    "                'num_threads': 8,\n",
    "                'tree_learner': 'voting',\n",
    "                'objective': 'multiclass',\n",
    "                'verbosity': 1,\n",
    "                'lambda_l2':2,\n",
    "                'num_classes': 11,\n",
    "                'seed':4567\n",
    "            }\n",
    "\n",
    "prev_year_ind = df_train[df_train['date'] < datetime.date(2019,1,1)].index\n",
    "now_ind = df_train[df_train['date'] >= datetime.date(2019,1,1)].index\n",
    "print(X.shape)\n",
    "X_feat = time_series_features(df_train,X.loc[prev_year_ind],X.loc[now_ind])\n",
    "y = df_train.loc[X_feat.index,'fire_type']\n",
    "train_data = lgb.Dataset(X_feat[model_features], label=y-1,\n",
    "                        feature_name=model_features,\n",
    "                        categorical_feature = [])\n",
    "print(X_feat[features].shape)\n",
    "print('number of features {}'.format(len(train_data.feature_name)))\n",
    "print('categorical features {}'.format(train_data.categorical_feature))\n",
    "\n",
    "lgb_model = lgb.train(params_lgb,train_data,num_boost_round=3200,feature_name=model_features) \n",
    "\n",
    "lgb_model.save_model('lgbm_model_3200_iter_{}_filtered_features-without_month.dmp'.format(len(model_features)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
